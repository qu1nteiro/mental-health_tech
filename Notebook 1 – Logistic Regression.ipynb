{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26628c5e-ebe9-4cb6-b287-0d3426e4951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and basic setup\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, RocCurveDisplay\n",
    "\n",
    "# Display options for cleaner outputs\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "# Visualization style\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87a650f-db97-4d49-aac0-93c581dcad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load CSV and get a quick overview\n",
    "df = pd.read_csv('survey.csv')\n",
    "\n",
    "# Show the dimensions of the dataset\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "\n",
    "# Display the first 6 rows\n",
    "display(df.head(6))\n",
    "\n",
    "# Display summary info about the dataset (column types, non-null counts, etc.)\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8cf974-d935-4edf-b0a6-181460ce80fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Exploratory Data Analysis (EDA)\n",
    "\n",
    "# List all columns\n",
    "print(\"Columns in the dataset:\", list(df.columns))\n",
    "\n",
    "# Show counts of each value in the target column ('treatment')\n",
    "print(\"\\nTarget value counts (treatment):\")\n",
    "print(df['treatment'].value_counts(dropna=False))\n",
    "\n",
    "# Calculate percentage of missing values per column\n",
    "missing_pct = df.isna().mean().sort_values(ascending=False) * 100\n",
    "print(\"\\nColumns with missing values (%):\")\n",
    "display(missing_pct[missing_pct > 0].round(2))\n",
    "\n",
    "# Visualize the distribution of the target variable\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.countplot(data=df, x='treatment', order=df['treatment'].value_counts().index)\n",
    "plt.title('Distribution of the target: treatment')\n",
    "plt.show()\n",
    "\n",
    "# Visualize the distribution of age\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(df['Age'].dropna(), bins=30)\n",
    "plt.title('Age distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1da50b-3943-4979-8427-d2b212c4822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Basic cleaning: Age, Gender normalization, and mapping Yes/No to 1/0\n",
    "df2 = df.copy()  # work on a copy to preserve original data\n",
    "\n",
    "# Age cleaning: convert to numeric and filter out unrealistic values\n",
    "\n",
    "df2['Age'] = pd.to_numeric(df2['Age'], errors='coerce')  # convert to numeric, invalids become NaN\n",
    "print(\"Before filtering: number of null ages =\", df2['Age'].isna().sum())\n",
    "\n",
    "# Optional: remove ages outside a realistic range (14-100)\n",
    "df2.loc[(df2['Age'] < 14) | (df2['Age'] > 100), 'Age'] = np.nan\n",
    "print(\"After filtering: number of null ages =\", df2['Age'].isna().sum())\n",
    "\n",
    "\n",
    "# Gender normalization: map variations to Male / Female / Other\n",
    "\n",
    "def clean_gender(x):\n",
    "    if pd.isna(x):\n",
    "        return 'Other'\n",
    "    s = str(x).strip().lower()\n",
    "    # common male variants\n",
    "    if s in ['male', 'm', 'man', 'male-ish', 'maile', 'mal', 'cis male', 'male (cis)']:\n",
    "        return 'Male'\n",
    "    # common female variants\n",
    "    if s in ['female', 'f', 'woman', 'female (cis)', 'cis female']:\n",
    "        return 'Female'\n",
    "    # anything else (including 'trans' or 'non-binary') as Other\n",
    "    return 'Other'\n",
    "\n",
    "df2['Gender_clean'] = df2['Gender'].apply(clean_gender)\n",
    "\n",
    "\n",
    "# Map binary Yes/No columns to 1/0\n",
    "\n",
    "binary_cols = ['self_employed', 'family_history', 'treatment', 'remote_work', 'tech_company']\n",
    "\n",
    "for col in binary_cols:\n",
    "    if col in df2.columns:\n",
    "        df2[col] = df2[col].map({'Yes': 1, 'No': 0})  # convert Yes/No to 1/0\n",
    "        # keep existing numeric values as-is\n",
    "        df2[col] = df2[col].fillna(df2[col]).infer_objects(copy=False)\n",
    "\n",
    "print(\"Basic cleaning done. Sample data:\")\n",
    "display(df2[['Age','Gender','Gender_clean','self_employed','family_history','treatment']].head())\n",
    "\n",
    "# Distribution of target variable\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.countplot(data=df2, x='treatment', order=df2['treatment'].value_counts().index)\n",
    "plt.title('Distribution of target: treatment')\n",
    "plt.show()\n",
    "\n",
    "# Age distribution\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(df2['Age'].dropna(), bins=30)\n",
    "plt.title('Age distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b56962-7c1e-431a-8ab9-a4a8a5144b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Initial features and simple new features\n",
    "df3 = df2.copy()  # work on a new copy to preserve previous cleaning\n",
    "\n",
    "# Example of a binary feature: long_hours (e.g., work hours > 50)\n",
    "# Note: if there is no 'hours' column in the dataset, we skip this part.\n",
    "# Here, we assume there is no 'hours' column, so we do not create it.\n",
    "\n",
    "# List of candidate features to consider for analysis/modeling\n",
    "candidate_features = [\n",
    "    'Age', \n",
    "    'Gender_clean',\n",
    "    'self_employed',\n",
    "    'family_history',\n",
    "    'work_interfere',   # categorical: 'Never','Rarely','Sometimes','Often'\n",
    "    'no_employees',     # categorical: company size\n",
    "    'remote_work',\n",
    "    'tech_company',\n",
    "    'benefits',\n",
    "    'care_options',\n",
    "    'wellness_program',\n",
    "    'seek_help',\n",
    "    'anonymity'\n",
    "]\n",
    "\n",
    "# Keep only the features that exist in the current dataframe\n",
    "candidate_features = [c for c in candidate_features if c in df3.columns]\n",
    "print(\"Candidate features used:\", candidate_features)\n",
    "\n",
    "# Quick peek at unique values for categorical features (or those with <20 unique values)\n",
    "for col in candidate_features:\n",
    "    if df3[col].dtype == 'object' or df3[col].nunique() < 20:\n",
    "        print(f\"\\n--- {col} unique values ---\")\n",
    "        print(df3[col].fillna('NA').value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da33c3c7-3f92-4fb1-a8a2-f14f0083135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Preprocessing Pipeline (!= Feature Engineering)\n",
    "\n",
    "# Separate numerical and categorical features\n",
    "num_features = [c for c in candidate_features if df3[c].dtype in ['int64','float64'] and c != 'treatment']\n",
    "cat_features = [c for c in candidate_features if c not in num_features]\n",
    "\n",
    "print(\"Numerical features:\", num_features)\n",
    "print(\"Categorical features:\", cat_features)\n",
    "\n",
    "# Numerical pipeline: impute missing values with median, then scale\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # fill missing values with median\n",
    "    ('scaler', StandardScaler())                    # standardize (mean=0, std=1)\n",
    "])\n",
    "\n",
    "# Categorical pipeline: impute missing values with a constant, then one-hot encode\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),  # fill missing with 'Missing'\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) # convert categories to 0/1 columns\n",
    "])\n",
    "\n",
    "# Combine pipelines using ColumnTransformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_features),\n",
    "    ('cat', cat_pipeline, cat_features)\n",
    "], remainder='drop', sparse_threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3885d3fe-8bf0-4e68-8dfb-e99c75be359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Prepare features (X) and target (y), then split into train/test sets\n",
    "\n",
    "# Select features and target\n",
    "X = df3[candidate_features].copy()\n",
    "y = df3['treatment'].map({'Yes': 1, 'No': 0}) if df3['treatment'].dtype == 'object' else df3['treatment']\n",
    "\n",
    "# Remove rows with missing target values\n",
    "mask = y.notna()\n",
    "X = X[mask]\n",
    "y = y[mask].astype(int)\n",
    "\n",
    "# Split data into training and test sets (80/20) with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,        # maintain target class distribution\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Show shapes and target distribution in training set\n",
    "print(\"Train/test shapes:\", X_train.shape, X_test.shape)\n",
    "print(\"Train target distribution:\", np.bincount(y_train)/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fca9119-e51e-4ed3-8db6-ba700899d7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Baseline Dummy model and Logistic Regression with cross-validation\n",
    "# Dummy baseline: always predict the most frequent class\n",
    "dummy = Pipeline([\n",
    "    ('preproc', preprocessor),\n",
    "    ('clf', DummyClassifier(strategy='most_frequent'))\n",
    "])\n",
    "dummy.fit(X_train, y_train)\n",
    "print(\"Baseline (most frequent) test accuracy:\", dummy.score(X_test, y_test))\n",
    "\n",
    "# Logistic Regression with cross-validation\n",
    "log_pipe = Pipeline([\n",
    "    ('preproc', preprocessor),\n",
    "    ('clf', LogisticRegression(\n",
    "        max_iter=1000,          # allow enough iterations for convergence\n",
    "        class_weight='balanced',# handle class imbalance\n",
    "        solver='liblinear'      # suitable for small/medium datasets\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 5-fold stratified cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores_f1 = cross_val_score(log_pipe, X_train, y_train, cv=cv, scoring='f1')\n",
    "print(\"Logistic Regression CV F1 score (mean ± std):\", scores_f1.mean().round(4), \"±\", scores_f1.std().round(4))\n",
    "\n",
    "# Fit on the full training set and evaluate on test set\n",
    "log_pipe.fit(X_train, y_train)\n",
    "y_pred = log_pipe.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression Test Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598ef69f-5e87-42a4-bbc1-9758eb992f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Feature Importance Analysis (Original Logistic Regression Model)\n",
    "# Get the original trained model from the pipeline\n",
    "model_orig = log_pipe.named_steps['clf']\n",
    "\n",
    "# Get the preprocessor\n",
    "preprocessor_orig = log_pipe.named_steps['preproc']\n",
    "\n",
    "# Get feature names\n",
    "onehot_features_orig = preprocessor_orig.transformers_[1][1].named_steps['onehot'].get_feature_names_out(cat_features)\n",
    "all_features_orig = num_features + list(onehot_features_orig)\n",
    "\n",
    "# Create a DataFrame for coefficients\n",
    "coefs_orig = model_orig.coef_[0]\n",
    "feature_importance_orig = pd.DataFrame({\n",
    "    'Feature': all_features_orig,\n",
    "    'Coefficient': coefs_orig,\n",
    "    'Abs_Coefficient': abs(coefs_orig)\n",
    "})\n",
    "\n",
    "# 5. Top 10 features by absolute impact\n",
    "print(\"Top 10 most important features (overall impact) - Original Model:\")\n",
    "display(feature_importance_orig.sort_values(by='Abs_Coefficient', ascending=False).head(10))\n",
    "\n",
    "# 6. Top 5 features indicating 'Treatment = Yes'\n",
    "print(\"\\nTop 5 features indicating 'Treatment = Yes':\")\n",
    "display(feature_importance_orig.sort_values(by='Coefficient', ascending=False).head(5))\n",
    "\n",
    "# 7. Top 5 features indicating 'Treatment = No'\n",
    "print(\"\\nTop 5 features indicating 'Treatment = No':\")\n",
    "display(feature_importance_orig.sort_values(by='Coefficient', ascending=True).head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd12377b-3750-468f-914e-98d1dba65b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Confusion Matrix (Original Model)\n",
    "# 1. Generate the confusion matrix\n",
    "# y_test: true labels, y_pred: predicted labels from the original model\n",
    "cm_original = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# 2. Create a display object for the confusion matrix\n",
    "disp_original = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm_original, \n",
    "    display_labels=['No', 'Yes']  # adjust labels to match your target\n",
    ")\n",
    "\n",
    "# 3. Plot the confusion matrix\n",
    "print(\"Confusion Matrix (Original Model - No Tuning):\")\n",
    "disp_original.plot(cmap=plt.cm.Blues, colorbar=False)\n",
    "disp_original.ax_.grid(False)  # turn off grid\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335f75c9-0e17-4ae7-b099-2778dbaf22ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: ROC Curve (Receiver Operating Characteristic) - Original Model\n",
    "# Create the canvas\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Draw the ROC curve for the original logistic regression model\n",
    "RocCurveDisplay.from_estimator(\n",
    "    log_pipe,   # original model\n",
    "    X_test,\n",
    "    y_test,\n",
    "    name='Logistic Regression (Original)',\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# Draw the \"chance\" line (random guess) on the same plot\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Chance (AUC = 0.50)')\n",
    "\n",
    "# Clean up and show\n",
    "ax.set_title('ROC Curve - Original Logistic Regression Model')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print the AUC score separately\n",
    "y_probs_orig = log_pipe.predict_proba(X_test)[:, 1]  # Probabilities for the \"Yes\" class\n",
    "auc_score_orig = roc_auc_score(y_test, y_probs_orig)\n",
    "print(f\"Area Under the Curve (AUC Score - Original Model): {auc_score_orig:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553f4eb2-c486-4f0e-9d2a-4cd021c66882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Model Error Analysis (Original Logistic Regression Model)\n",
    "\n",
    "# Make predictions on the test set using the original model\n",
    "y_pred_orig = log_pipe.predict(X_test)\n",
    "\n",
    "# Identify errors\n",
    "errors_mask_orig = (y_pred_orig != y_test)\n",
    "X_test_errors_orig = X_test[errors_mask_orig]\n",
    "y_test_errors_orig = y_test[errors_mask_orig]\n",
    "y_pred_errors_orig = y_pred_orig[errors_mask_orig]\n",
    "\n",
    "print(f\"Total errors: {errors_mask_orig.sum()} / {len(y_test)} ({errors_mask_orig.sum()/len(y_test)*100:.1f}%)\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# eparate false positives and false negatives\n",
    "false_positives_orig = (y_pred_errors_orig == 1) & (y_test_errors_orig == 0)\n",
    "false_negatives_orig = (y_pred_errors_orig == 0) & (y_test_errors_orig == 1)\n",
    "\n",
    "print(f\"\\nFalse Positives: {false_positives_orig.sum()} cases (Predicted 'Yes' but was 'No')\")\n",
    "print(f\"False Negatives: {false_negatives_orig.sum()} cases (Predicted 'No' but was 'Yes')\")\n",
    "\n",
    "# Profile False Positives\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROFILE OF FALSE POSITIVES:\")\n",
    "if false_positives_orig.sum() > 0:\n",
    "    fp_data_orig = X_test_errors_orig[false_positives_orig]\n",
    "    print(\"\\nAverage Age:\", fp_data_orig['Age'].mean())\n",
    "    print(\"\\nGender:\")\n",
    "    print(fp_data_orig['Gender_clean'].value_counts())\n",
    "    print(\"\\nWork Interfere:\")\n",
    "    print(fp_data_orig['work_interfere'].value_counts())\n",
    "\n",
    "# Profile False Negatives\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROFILE OF FALSE NEGATIVES:\")\n",
    "if false_negatives_orig.sum() > 0:\n",
    "    fn_data_orig = X_test_errors_orig[false_negatives_orig]\n",
    "    print(\"\\nAverage Age:\", fn_data_orig['Age'].mean())\n",
    "    print(\"\\nGender:\")\n",
    "    print(fn_data_orig['Gender_clean'].value_counts())\n",
    "    print(\"\\nWork Interfere:\")\n",
    "    print(fn_data_orig['work_interfere'].value_counts())\n",
    "\n",
    "# Visualization: compare distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Age histogram\n",
    "axes[0].hist(X_test[~errors_mask_orig]['Age'].dropna(), bins=20, alpha=0.5, label='Correct', color='green')\n",
    "axes[0].hist(X_test_errors_orig['Age'].dropna(), bins=20, alpha=0.5, label='Errors', color='red')\n",
    "axes[0].set_xlabel('Age')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Age Distribution: Correct vs Errors')\n",
    "axes[0].legend()\n",
    "\n",
    "# Work Interfere bar plot\n",
    "work_counts_correct_orig = X_test[~errors_mask_orig]['work_interfere'].value_counts()\n",
    "work_counts_errors_orig = X_test_errors_orig['work_interfere'].value_counts()\n",
    "\n",
    "x = range(len(work_counts_correct_orig))\n",
    "width = 0.35\n",
    "axes[1].bar([i - width/2 for i in x], work_counts_correct_orig.values, width, label='Correct', color='green', alpha=0.7)\n",
    "axes[1].bar([i + width/2 for i in x], work_counts_errors_orig.values, width, label='Errors', color='red', alpha=0.7)\n",
    "axes[1].set_xlabel('Work Interfere')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Work Interfere: Correct vs Errors')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(work_counts_correct_orig.index, rotation=45)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd30c2e-e40e-4ce5-b540-eb248bb545e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Definition of the tuning pipeline\n",
    "\n",
    "# Create the full pipeline\n",
    "# Combine the preprocessor (already defined) with a Logistic Regression model\n",
    "# Note: solver='liblinear' is good for small datasets and supports L1/L2 regularization\n",
    "#       max_iter=1000 avoids convergence warnings\n",
    "pipe_lr = Pipeline([\n",
    "    ('pre', preprocessor), \n",
    "    ('model', LogisticRegression(solver='liblinear', max_iter=1000))\n",
    "])\n",
    "\n",
    "# Define the grid of parameters to test\n",
    "# We will test different regularization strengths (C) and types of regularization (penalty)\n",
    "# 'model__C' means: set the parameter 'C' in the pipeline step named 'model'\n",
    "param_grid = {\n",
    "    'model__penalty': ['l1', 'l2'],\n",
    "    'model__C': [0.001, 0.01, 0.1, 1, 10, 100] \n",
    "}\n",
    "\n",
    "# 3. Set up GridSearchCV\n",
    "# cv=5 -> 5-fold cross-validation\n",
    "# scoring='f1' -> optimize for F1 score\n",
    "# n_jobs=-1 -> use all available CPU cores for faster computation\n",
    "grid_lr = GridSearchCV(pipe_lr, param_grid, cv=5, scoring='f1', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd425700-b0e0-4a49-a7c0-5213fe7428e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Run the hyperparameter tuning (GridSearch)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the GridSearchCV on the training data\n",
    "# It will try all combinations in param_grid using X_train and y_train\n",
    "grid_lr.fit(X_train, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Print the time it took\n",
    "print(f\"GridSearch took {end_time - start_time:.2f} seconds.\")\n",
    "print(\"---\")\n",
    "\n",
    "# Print the best F1 score found during cross-validation\n",
    "print(f\"Best F1 score (from CV): {grid_lr.best_score_:.4f}\")\n",
    "\n",
    "# Print the combination of parameters that gave the best score\n",
    "print(\"Best parameters found:\")\n",
    "print(grid_lr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcc312b-0043-4a6d-a123-d9a06b7667c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Evaluate the optimized model on the test set\n",
    "# Get the best estimator found by GridSearchCV\n",
    "best_lr = grid_lr.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_lr_tuned = best_lr.predict(X_test)\n",
    "\n",
    "# Display the classification report\n",
    "print(\"Classification Report (Optimized Model on Test Set):\")\n",
    "print(classification_report(y_test, y_pred_lr_tuned, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2948d0c9-1972-4f90-a0e0-ee6ba2c4cbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Feature Importance Analysis\n",
    "\n",
    "# Get the trained model from the pipeline\n",
    "model = best_lr.named_steps['model']\n",
    "\n",
    "# Get the preprocessor to retrieve feature names\n",
    "preprocessor = best_lr.named_steps['pre']\n",
    "\n",
    "# Get the feature names\n",
    "# The preprocessor has two transformers ('num' and 'cat')\n",
    "# The 'cat' transformer contains the one-hot encoder\n",
    "onehot_features = preprocessor.transformers_[1][1].named_steps['onehot'].get_feature_names_out(cat_features)\n",
    "\n",
    "# Combine numeric features + one-hot encoded categorical features\n",
    "# (The order must match the ColumnTransformer: numeric first, then categorical)\n",
    "all_features = num_features + list(onehot_features)\n",
    "\n",
    "# Create a nice DataFrame to view results\n",
    "coefs = model.coef_[0]\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': all_features,\n",
    "    'Coefficient': coefs,\n",
    "    'Abs_Coefficient': abs(coefs)  # absolute value to sort by importance\n",
    "})\n",
    "\n",
    "# Show the top 10 most impactful features (positive or negative)\n",
    "print(\"Top 10 most important features (overall impact):\")\n",
    "display(feature_importance.sort_values(by='Abs_Coefficient', ascending=False).head(10))\n",
    "\n",
    "# Show the top 5 features that most indicate 'Treatment = Yes'\n",
    "print(\"\\nTop 5 features that increase the likelihood of 'Treatment = Yes':\")\n",
    "display(feature_importance.sort_values(by='Coefficient', ascending=False).head(5))\n",
    "\n",
    "# Show the top 5 features that most indicate 'Treatment = No'\n",
    "print(\"\\nTop 5 features that increase the likelihood of 'Treatment = No':\")\n",
    "display(feature_importance.sort_values(by='Coefficient', ascending=True).head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3f98c5-0991-4c1d-b5e3-4e204734b5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Confusion Matrix\n",
    "# Generate the matrix\n",
    "cm = confusion_matrix(y_test, y_pred_lr_tuned)\n",
    "\n",
    "# Draw the matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                              display_labels=['No', 'Yes'])\n",
    "\n",
    "# Show the plot\n",
    "print(\"Confusion Matrix (Optimized Model):\")\n",
    "\n",
    "# We add 'colorbar=False' to remove the scale on the right\n",
    "disp.plot(cmap=plt.cm.Blues, colorbar=False) \n",
    "\n",
    "# We access the plot's 'axis' (ax_) and turn off the grid\n",
    "disp.ax_.grid(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a0b1dd-06ce-40ab-bd14-a97bc34f17f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: ROC Curve (Receiver Operating Characteristic)\n",
    "# First, we create the \"canvas\" (the axis)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Now, we tell RocCurveDisplay to draw itself on that canvas\n",
    "# It uses .predict_proba() automatically to get the \"confidence\"\n",
    "RocCurveDisplay.from_estimator(\n",
    "    best_lr, # tuned model from GridSearch\n",
    "    X_test,\n",
    "    y_test,\n",
    "    name='Logistic Regression (Tuned)', # Name for the legend\n",
    "    ax=ax # use the canvas we created\n",
    ")\n",
    "\n",
    "# Finally, we draw the \"chance\" (Dummy) line on the SAME canvas\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Chance (AUC = 0.50)') # 'k--' is a black dashed line\n",
    "    \n",
    "# 4. Clean up and show\n",
    "ax.set_title('ROC Curve (Optimized Model)')\n",
    "ax.legend() # Activates the legend (which shows the model's AUC)\n",
    "plt.show()\n",
    "\n",
    "# 5. Print the AUC score separately\n",
    "# The plot already shows it, but this way you have the number\n",
    "y_probs = best_lr.predict_proba(X_test)[:, 1] # Probabilities for the \"Yes\" class only\n",
    "auc_score = roc_auc_score(y_test, y_probs)\n",
    "print(f\"Area Under the Curve (AUC Score): {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b2f599-38c4-4b16-b2bc-d70ef92a1594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: Model Error Analysis\n",
    "# Identify which predictions were wrong\n",
    "errors_mask = (y_pred_lr_tuned != y_test)\n",
    "\n",
    "# Get the features of the misclassified samples\n",
    "X_test_errors = X_test[errors_mask]\n",
    "y_test_errors = y_test[errors_mask]\n",
    "y_pred_errors = y_pred_lr_tuned[errors_mask]\n",
    "\n",
    "print(f\"Total errors: {errors_mask.sum()} / {len(y_test)} ({errors_mask.sum()/len(y_test)*100:.1f}%)\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Separate the two types of errors\n",
    "false_positives = (y_pred_errors == 1) & (y_test_errors == 0)\n",
    "false_negatives = (y_pred_errors == 0) & (y_test_errors == 1)\n",
    "\n",
    "print(f\"\\nFalse Positives: {false_positives.sum()} cases\")\n",
    "print(\"   (Model predicted 'Yes' but it was 'No')\")\n",
    "print(f\"\\nFalse Negatives: {false_negatives.sum()} cases\")\n",
    "print(\"   (Model predicted 'No' but it was 'Yes')\")\n",
    "\n",
    "#  Analyze characteristics of false positives\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROFILE OF FALSE POSITIVES:\")\n",
    "print(\"=\"*60)\n",
    "if false_positives.sum() > 0:\n",
    "    fp_data = X_test_errors[false_positives]\n",
    "    print(\"\\nAverage Age:\", fp_data['Age'].mean())\n",
    "    print(\"\\nGender:\")\n",
    "    print(fp_data['Gender_clean'].value_counts())\n",
    "    print(\"\\nWork Interfere:\")\n",
    "    print(fp_data['work_interfere'].value_counts())\n",
    "\n",
    "# Analyze characteristics of false negatives\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROFILE OF FALSE NEGATIVES:\")\n",
    "print(\"=\"*60)\n",
    "if false_negatives.sum() > 0:\n",
    "    fn_data = X_test_errors[false_negatives]\n",
    "    print(\"\\nAverage Age:\", fn_data['Age'].mean())\n",
    "    print(\"\\nGender:\")\n",
    "    print(fn_data['Gender_clean'].value_counts())\n",
    "    print(\"\\nWork Interfere:\")\n",
    "    print(fn_data['work_interfere'].value_counts())\n",
    "\n",
    "# VISUALIZATION: Compare distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Age distribution\n",
    "axes[0].hist(X_test[~errors_mask]['Age'].dropna(), bins=20, alpha=0.5, label='Correct', color='green')\n",
    "axes[0].hist(X_test_errors['Age'].dropna(), bins=20, alpha=0.5, label='Errors', color='red')\n",
    "axes[0].set_xlabel('Age')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Age Distribution: Correct vs Errors')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot 2: Work Interfere\n",
    "work_counts_correct = X_test[~errors_mask]['work_interfere'].value_counts()\n",
    "work_counts_errors = X_test_errors['work_interfere'].value_counts()\n",
    "\n",
    "x = range(len(work_counts_correct))\n",
    "width = 0.35\n",
    "axes[1].bar([i - width/2 for i in x], work_counts_correct.values, width, label='Correct', color='green', alpha=0.7)\n",
    "axes[1].bar([i + width/2 for i in x], work_counts_errors.values, width, label='Errors', color='red', alpha=0.7)\n",
    "axes[1].set_xlabel('Work Interfere')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Work Interfere: Correct vs Errors')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(work_counts_correct.index, rotation=45)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5798a799-65b7-46c3-b62c-3862aec2ed37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
