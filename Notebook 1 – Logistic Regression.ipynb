{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a375d8-50c6-4150-bc5c-082cdf81ad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell One: imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, RocCurveDisplay\n",
    "\n",
    "# display options\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 200)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"Imports ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28076334-f8ad-43a5-82a5-bccdb7830907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell two: load and view CSV  \n",
    "df = pd.read_csv('survey.csv')\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head(6))\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86618059-9cae-48c6-a93c-6c76aa8bc8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the 10 largest and smallest values from column 'Age'\n",
    "print(df['Age'].sort_values(ascending=False).head(10))\n",
    "print(df['Age'].sort_values(ascending=True).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb4c7ca-93a7-4ef2-a95a-8982eb21491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell three:Exploratory Data Analysis\n",
    "print(\"Columns:\", list(df.columns))\n",
    "print(\"\\nTarget value counts (treatment):\")\n",
    "print(df['treatment'].value_counts(dropna=False))\n",
    "\n",
    "# Missing values %\n",
    "missing_pct = df.isna().mean().sort_values(ascending=False) * 100\n",
    "display(missing_pct[missing_pct>0].round(2))\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.countplot(data=df, x='treatment', order=df['treatment'].value_counts().index)\n",
    "plt.title('Distribution of target: treatment')\n",
    "plt.show()\n",
    "\n",
    "# Age\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "# 1. Outliers removal\n",
    "df2 = df[(df['Age'] >= 14) & (df['Age'] <= 100)].copy()\n",
    "\n",
    "# 2. Usamos esse novo DataFrame para o gráfico\n",
    "sns.histplot(df2['Age'].dropna(), bins=30)\n",
    "plt.title('Age distribution (Filtered 0-100 years)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062a0015-6abf-40e3-b926-bdd74e08f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Gender: map to Male / Female / Other\n",
    "def clean_gender(x):\n",
    "    if pd.isna(x): return 'Other'\n",
    "    s = str(x).strip().lower()\n",
    "    # common cases\n",
    "    if s in ['male', 'm', 'man', 'male-ish', 'maile', 'mal', 'cis male', 'male (cis)']: return 'Male'\n",
    "    if s in ['female', 'f', 'woman', 'female (cis)', 'cis female']: return 'Female'\n",
    "    # other\n",
    "    return 'Other'\n",
    "\n",
    "df2['Gender_clean'] = df2['Gender'].apply(clean_gender)\n",
    "\n",
    "# Map binary answers from 'Yes'/'No' to 1/0 for columns that have them:\n",
    "bin_cols = ['self_employed','family_history','treatment','remote_work','tech_company']\n",
    "for c in bin_cols:\n",
    "    if c in df2.columns:\n",
    "        df2.loc[:, c] = df2[c].replace({'Yes':1, 'No':0})\n",
    "print(\"Cleaned basic columns. Sample:\")\n",
    "display(df2[['Age','Gender','Gender_clean','self_employed','family_history','treatment']].head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13ba80b-e1e0-4075-a050-60864aaaea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Initial features and simple new features\n",
    "df3 = df2.copy()\n",
    "\n",
    "candidate_features = [\n",
    "    'Age', \n",
    "    'Gender_clean',\n",
    "    'self_employed',\n",
    "    'family_history',\n",
    "    'work_interfere',   # categorical: 'Never','Rarely','Sometimes','Often'\n",
    "    'no_employees',     # categorical: size of company\n",
    "    'remote_work',\n",
    "    'tech_company',\n",
    "    'benefits',\n",
    "    'care_options',\n",
    "    'wellness_program',\n",
    "    'seek_help',\n",
    "    'anonymity'\n",
    "]\n",
    "\n",
    "# See which columns exist in the df.\n",
    "candidate_features = [c for c in candidate_features if c in df3.columns]\n",
    "print(\"Candidate features used:\", candidate_features)\n",
    "\n",
    "# quick peek at categorical uniques for those columns\n",
    "for c in candidate_features:\n",
    "    if df3[c].dtype=='object' or df3[c].nunique() < 20:\n",
    "        print(f\"\\n--- {c} unique values ---\")\n",
    "        print(df3[c].fillna('NA').value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad33451-e1e7-405f-bc2b-9a367a52b65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: preprocessing pipeline: imputers, scaler e one-hot\n",
    "\n",
    "num_features = [c for c in candidate_features if df3[c].dtype in ['int64','float64'] and c!='treatment']\n",
    "cat_features = [c for c in candidate_features if c not in num_features]\n",
    "\n",
    "print(\"num_features:\", num_features)\n",
    "print(\"cat_features:\", cat_features)\n",
    "\n",
    "# Numeric pipeline\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_features),\n",
    "    ('cat', cat_pipeline, cat_features)\n",
    "], remainder='drop', sparse_threshold=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012950a4-4026-491f-9fd9-b031c6e9ca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Prepare X,y and separate training/test (ROBUST VERSION)\n",
    "\n",
    "X = df3[candidate_features].copy()\n",
    "\n",
    "# --- CORRECTION IS HERE ---\n",
    "# A lógica antiga estava a criar 100% de NaNs.\n",
    "# Vamos usar a lógica \"À Prova de Bala\" que já descobrimos:\n",
    "\n",
    "# 1. Primeiro, substitui o texto 'Yes'/'No' (se existir) por 1/0\n",
    "y_temp = df3['treatment'].replace({'Yes': 1, 'No': 0})\n",
    "\n",
    "# 2. DEPOIS, usa pd.to_numeric para forçar tudo o resto\n",
    "#    Isto converte 1.0 -> 1.0, '1' -> 1.0, \"Don't Know\" -> NaN\n",
    "y = pd.to_numeric(y_temp, errors='coerce')\n",
    "# --- FIM DA CORREÇÃO ---\n",
    "\n",
    "\n",
    "# Agora, removemos as poucas linhas que podem ter sido \"Don't Know\" (NaN)\n",
    "mask = y.notna()\n",
    "X = X[mask]\n",
    "y = y[mask].astype(int) # .astype(int) agora funciona porque não há NaNs\n",
    "\n",
    "print(f\"Shape after cleaning NaNs from target: X={X.shape}, y={y.shape}\")\n",
    "\n",
    "# Isto agora vai funcionar\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train/test shapes:\", X_train.shape, X_test.shape)\n",
    "print(\"Train target dist:\", np.bincount(y_train)/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0255b196-4372-408e-8fcd-736620284901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 (CORRIGIDA): preprocessing pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer # <-- IMPORTAR ISTO\n",
    "\n",
    "num_features = [c for c in candidate_features if df3[c].dtype in ['int64','float64'] and c!='treatment']\n",
    "cat_features = [c for c in candidate_features if c not in num_features]\n",
    "\n",
    "print(\"num_features:\", num_features)\n",
    "print(\"cat_features:\", cat_features)\n",
    "\n",
    "# Numeric pipeline\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline (COM A CORREÇÃO)\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('to_string', FunctionTransformer(lambda x: x.astype(str))), # <-- A CORREÇÃO MÁGICA\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_features),\n",
    "    ('cat', cat_pipeline, cat_features)\n",
    "], remainder='drop', sparse_threshold=0)\n",
    "\n",
    "print(\"Preprocessor (com o fix .astype(str)) criado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a65ac4d-cbf0-4788-867f-b0ced7fed43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8.1: Confusion Matrix (Original Model)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# 1. Generate the matrix\n",
    "# (y_test is the truth, y_pred is the prediction from your original model)\n",
    "cm_original = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# 2. Draw the matrix\n",
    "disp_original = ConfusionMatrixDisplay(confusion_matrix=cm_original, \n",
    "                                       display_labels=['No', 'Yes'])\n",
    "\n",
    "# 3. Show the plot\n",
    "print(\"Confusion Matrix (Original Model - No Tuning):\")\n",
    "disp_original.plot(cmap=plt.cm.Blues, colorbar=False)\n",
    "disp_original.ax_.grid(False) # Turn off the grid\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd761981-226c-441e-bfec-7a2385dd5e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Imports and definition of the Tuning Pipeline\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# 1. Create the COMPLETE pipeline\n",
    "# Joins your 'preprocessor' (which you already have) with the 'LogisticRegression'\n",
    "#\n",
    "# Note: I'm adding solver='liblinear' and max_iter=1000\n",
    "# 'liblinear' is good for small datasets and handles L1/L2 regularization well\n",
    "# A higher 'max_iter' avoids annoying convergence warnings.\n",
    "\n",
    "pipe_lr = Pipeline([\n",
    "    ('pre', preprocessor), \n",
    "    ('model', LogisticRegression(solver='liblinear', max_iter=1000))\n",
    "])\n",
    "\n",
    "# 2. Define the \"grid\" of parameters we want to test\n",
    "# We will test the regularization strength (C) and the regularization type (penalty)\n",
    "#\n",
    "# 'model__C' -> The '__' (double underscore) tells the Pipeline: \n",
    "# \"I want you to pass the 'C' parameter to the step I called 'model'\"\n",
    "\n",
    "param_grid = {\n",
    "    'model__penalty': ['l1', 'l2'],\n",
    "    'model__C': [0.001, 0.01, 0.1, 1, 10, 100] \n",
    "}\n",
    "\n",
    "# 3. Configure the GridSearch\n",
    "# cv=5 -> 5-fold cross-validation (as you had already done)\n",
    "# scoring='f1' -> We want to optimize for the F1 score\n",
    "# n_jobs=-1 -> Use all your PC's processors. It will be fast.\n",
    "grid_lr = GridSearchCV(pipe_lr, param_grid, cv=5, scoring='f1', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4424ac-6da4-4aad-bc71-442571f4f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Run the Tuning\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the GridSearch (this might take a few seconds)\n",
    "# It will use X_train and y_train\n",
    "grid_lr.fit(X_train, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"GridSearch took {end_time - start_time:.2f} seconds.\")\n",
    "print(\"---\")\n",
    "print(f\"Best F1 score (in CV): {grid_lr.best_score_:.4f}\")\n",
    "print(\"Best parameters found:\")\n",
    "print(grid_lr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f3d7d5-135b-45e3-b318-7380ed289250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Evaluate the optimized model on the test set\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get the best estimator that GridSearch found\n",
    "best_lr = grid_lr.best_estimator_\n",
    "\n",
    "# Make predictions on X_test\n",
    "y_pred_lr_tuned = best_lr.predict(X_test)\n",
    "\n",
    "print(\"Classification Report (Optimized Model on Test Set):\")\n",
    "print(classification_report(y_test, y_pred_lr_tuned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b39ec59-53f0-45da-96b1-e3cd7857564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Feature Importance Analysis\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Get the trained model (it's inside the pipeline)\n",
    "model = best_lr.named_steps['model']\n",
    "\n",
    "# 2. Get the preprocessor (so we know the column names)\n",
    "preprocessor = best_lr.named_steps['pre']\n",
    "\n",
    "# 3. Get the feature names\n",
    "# The preprocessor has 2 transformers ('num' and 'cat')\n",
    "# The 'cat' transformer has the 'onehot' encoder inside it\n",
    "onehot_features = preprocessor.transformers_[1][1].named_steps['onehot'].get_feature_names_out(cat_features)\n",
    "\n",
    "# Join the numeric + one-hot feature names\n",
    "# (The order MUST be the same as in ColumnTransformer: num, then cat)\n",
    "all_features = num_features + list(onehot_features)\n",
    "\n",
    "# 4. Create a nice DataFrame to see the results\n",
    "coefs = model.coef_[0]\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': all_features,\n",
    "    'Coefficient': coefs, # 'Coeficiente' translated\n",
    "    'Abs_Coef': abs(coefs) # Absolute value to sort by importance\n",
    "})\n",
    "\n",
    "# 5. Show the 10 most impactful features (positive or negative)\n",
    "print(\"Most important features (total impact):\")\n",
    "display(feature_importance.sort_values(by='Abs_Coef', ascending=False).head(10))\n",
    "\n",
    "# 6. Show the top 5 features that lead to \"Yes\" (treatment=1)\n",
    "print(\"\\nFeatures that most indicate 'Treatment = Yes':\")\n",
    "display(feature_importance.sort_values(by='Coefficient', ascending=False).head(5))\n",
    "\n",
    "# 7. Show the top 5 features that lead to \"No\" (treatment=0)\n",
    "print(\"\\nFeatures that most indicate 'Treatment = No':\")\n",
    "display(feature_importance.sort_values(by='Coefficient', ascending=True).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5973d1-81a6-4b2d-bf02-8c6d49cf2bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Confusion Matrix (Clean Version)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# 1. Generate the matrix (this stays the same)\n",
    "cm = confusion_matrix(y_test, y_pred_lr_tuned)\n",
    "\n",
    "# 2. Draw the matrix (this stays the same)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                              display_labels=['No', 'Yes'])\n",
    "\n",
    "# 3. Show the plot\n",
    "print(\"Confusion Matrix (Optimized Model):\")\n",
    "\n",
    "#  Add 'colorbar=False' to remove the scale on the right\n",
    "disp.plot(cmap=plt.cm.Blues, colorbar=False) \n",
    "\n",
    "# The plot's 'axis' (ax_) and turn off the grid\n",
    "disp.ax_.grid(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d11a090-ca0c-46a0-ae12-5e53cfe386ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: ROC Curve (Receiver Operating Characteristic)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import RocCurveDisplay, roc_auc_score\n",
    "\n",
    "# 1. First, let's create the \"canvas\" (the axis)\n",
    "# This allows us to draw both lines on the same plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# 2. Now, we tell RocCurveDisplay to draw itself ON THAT canvas\n",
    "# It uses .predict_proba() automatically to get the \"confidence\"\n",
    "RocCurveDisplay.from_estimator(\n",
    "    best_lr, # Your tuned model from GridSearch\n",
    "    X_test,\n",
    "    y_test,\n",
    "    name='Logistic Regression (Tuned)', # Name for the legend\n",
    "    ax=ax # Tells it to use the canvas we created\n",
    ")\n",
    "\n",
    "# 3. Finally, we draw the \"chance\" (Dummy) line on the SAME canvas\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Chance (AUC = 0.50)') # 'k--' is a black dashed line\n",
    "    \n",
    "# 4. Clean up and show\n",
    "ax.set_title('ROC Curve (Optimized Model)')\n",
    "ax.legend() # Activates the legend (which shows your model's AUC)\n",
    "plt.show()\n",
    "\n",
    "# 5. (Optional) Print the AUC score separately\n",
    "# The plot already shows it, but this way you have the number\n",
    "y_probs = best_lr.predict_proba(X_test)[:, 1] # Probabilities for the \"Yes\" class only\n",
    "auc_score = roc_auc_score(y_test, y_probs)\n",
    "print(f\"Area Under the Curve (AUC Score): {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26628c5e-ebe9-4cb6-b287-0d3426e4951e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
