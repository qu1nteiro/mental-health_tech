{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26628c5e-ebe9-4cb6-b287-0d3426e4951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√©lula 1: imports e op√ß√µes\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, RocCurveDisplay\n",
    "\n",
    "# display options\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 200)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"Imports ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87a650f-db97-4d49-aac0-93c581dcad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√©lula 2: carregar CSV e vis√£o r√°pida\n",
    "df = pd.read_csv('survey.csv')\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head(6))\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8cf974-d935-4edf-b0a6-181460ce80fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√©lula 3: Exploratory Data Analysis\n",
    "print(\"Colunas:\", list(df.columns))\n",
    "print(\"\\nTarget value counts (treatment):\")\n",
    "print(df['treatment'].value_counts(dropna=False))\n",
    "\n",
    "# Missing values %\n",
    "missing_pct = df.isna().mean().sort_values(ascending=False) * 100\n",
    "display(missing_pct[missing_pct>0].round(2))\n",
    "\n",
    "# visualizar algumas distribui√ß√µes\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.countplot(data=df, x='treatment', order=df['treatment'].value_counts().index)\n",
    "plt.title('Distribution of target: treatment')\n",
    "plt.show()\n",
    "\n",
    "# idade\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(df['Age'].dropna(), bins=30)\n",
    "plt.title('Age distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1da50b-3943-4979-8427-d2b212c4822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√©lula 4: clean basic - age, gender normalisation, map Yes/No\n",
    "df2 = df.copy()\n",
    "\n",
    "# Age cleaning: converter para num√©rico e filtrar valores an√≥malos\n",
    "df2['Age'] = pd.to_numeric(df2['Age'], errors='coerce')\n",
    "print(\"Antes: n null age =\", df2['Age'].isna().sum())\n",
    "# remover idades absurdas (opcional): manter 14-100\n",
    "df2.loc[(df2['Age'] < 14) | (df2['Age'] > 100), 'Age'] = np.nan\n",
    "print(\"Depois filtro: n null age =\", df2['Age'].isna().sum())\n",
    "\n",
    "# Normalizar Gender: map para Male / Female / Other\n",
    "def clean_gender(x):\n",
    "    if pd.isna(x): return 'Other'\n",
    "    s = str(x).strip().lower()\n",
    "    # casos comuns\n",
    "    if s in ['male', 'm', 'man', 'male-ish', 'maile', 'mal', 'cis male', 'male (cis)']: return 'Male'\n",
    "    if s in ['female', 'f', 'woman', 'female (cis)', 'cis female']: return 'Female'\n",
    "    # se contiver 'trans' ou 'non' agrupar como Other\n",
    "    return 'Other'\n",
    "\n",
    "df2['Gender_clean'] = df2['Gender'].apply(clean_gender)\n",
    "\n",
    "# Map binary answers from 'Yes'/'No' to 1/0 for columns that have them:\n",
    "bin_cols = ['self_employed','family_history','treatment','remote_work','tech_company']\n",
    "for c in bin_cols:\n",
    "    if c in df2.columns:\n",
    "        df2[c] = df2[c].map({'Yes':1, 'No':0})\n",
    "        df2[c] = df2[c].fillna(df2[c]).infer_objects(copy=False)  # se j√° forem 0/1 mant√©m\n",
    "print(\"Cleaned basic columns. Sample:\")\n",
    "display(df2[['Age','Gender','Gender_clean','self_employed','family_history','treatment']].head())\n",
    "\n",
    "# visualizar algumas distribui√ß√µes\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.countplot(data=df2, x='treatment', order=df2['treatment'].value_counts().index)\n",
    "plt.title('Distribution of target: treatment')\n",
    "plt.show()\n",
    "\n",
    "# idade\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(df2['Age'].dropna(), bins=30)\n",
    "plt.title('Age distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b56962-7c1e-431a-8ab9-a4a8a5144b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√©lula 5: features iniciais e novas features simples\n",
    "df3 = df2.copy()\n",
    "\n",
    "# Criar feature bin√°ria: long_hours (ex.: horas de trabalho > 50)\n",
    "# Note: se n√£o existir coluna de horas, ignora esta parte. Aqui assumimos que n√£o h√° 'hours' neste dataset -> n√£o criar.\n",
    "# Exemplos de features a usar (ajusta conforme interesse):\n",
    "candidate_features = [\n",
    "    'Age', \n",
    "    'Gender_clean',\n",
    "    'self_employed',\n",
    "    'family_history',\n",
    "    'work_interfere',   # categorical: 'Never','Rarely','Sometimes','Often'\n",
    "    'no_employees',     # categorical: size of company\n",
    "    'remote_work',\n",
    "    'tech_company',\n",
    "    'benefits',\n",
    "    'care_options',\n",
    "    'wellness_program',\n",
    "    'seek_help',\n",
    "    'anonymity'\n",
    "]\n",
    "\n",
    "# Ver que colunas existem no df\n",
    "candidate_features = [c for c in candidate_features if c in df3.columns]\n",
    "print(\"Candidate features used:\", candidate_features)\n",
    "\n",
    "# quick peek at categorical uniques for those columns\n",
    "for c in candidate_features:\n",
    "    if df3[c].dtype=='object' or df3[c].nunique() < 20:\n",
    "        print(f\"\\n--- {c} unique values ---\")\n",
    "        print(df3[c].fillna('NA').value_counts().head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da33c3c7-3f92-4fb1-a8a2-f14f0083135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√©lula 6: Preprocessing Pipeline (SEM Feature Engineering)\n",
    "\n",
    "# USAR LISTA ANTIGA\n",
    "num_features = [c for c in candidate_features if df3[c].dtype in ['int64','float64'] and c!='treatment']\n",
    "cat_features = [c for c in candidate_features if c not in num_features]\n",
    "\n",
    "print(\"num_features:\", num_features)\n",
    "print(\"cat_features:\", cat_features)\n",
    "\n",
    "# Pipelines (igual)\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_features),\n",
    "    ('cat', cat_pipeline, cat_features)\n",
    "], remainder='drop', sparse_threshold=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3885d3fe-8bf0-4e68-8dfb-e99c75be359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√©lula 7: preparar X,y e separar treino/teste\n",
    "X = df3[candidate_features].copy()\n",
    "y = df3['treatment'].map({'Yes':1,'No':0}) if df3['treatment'].dtype=='object' else df3['treatment']\n",
    "# Se houver NaNs no target, remover\n",
    "mask = y.notna()\n",
    "X = X[mask]\n",
    "y = y[mask].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train/test shapes:\", X_train.shape, X_test.shape)\n",
    "print(\"Train target dist:\", np.bincount(y_train)/len(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fca9119-e51e-4ed3-8db6-ba700899d7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√©lula 8: baseline Dummy e Logistic regression com cross-validation\n",
    "# Dummy baseline\n",
    "dummy = Pipeline([('preproc', preprocessor), ('clf', DummyClassifier(strategy='most_frequent'))])\n",
    "dummy.fit(X_train, y_train)\n",
    "print(\"Baseline most frequent test score (accuracy):\", dummy.score(X_test, y_test))\n",
    "\n",
    "# Logistic CV\n",
    "log_pipe = Pipeline([('preproc', preprocessor), ('clf', LogisticRegression(max_iter=1000, class_weight='balanced', solver='liblinear'))])\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores_f1 = cross_val_score(log_pipe, X_train, y_train, cv=cv, scoring='f1')\n",
    "print(\"Logistic CV F1 mean ¬± std:\", scores_f1.mean().round(4), scores_f1.std().round(4))\n",
    "\n",
    "# Fit on full train and evaluate on test\n",
    "log_pipe.fit(X_train, y_train)\n",
    "y_pred = log_pipe.predict(X_test)\n",
    "print(\"Logistic Test classification report:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd12377b-3750-468f-914e-98d1dba65b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√©lula 9: Matriz de Confus√£o (Modelo Original)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# 1. Gerar a matriz\n",
    "# (y_test √© a verdade, y_pred √© a previs√£o do teu modelo original)\n",
    "cm_original = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# 2. Desenhar a matriz\n",
    "disp_original = ConfusionMatrixDisplay(confusion_matrix=cm_original, \n",
    "                                       display_labels=['No', 'Yes'])\n",
    "\n",
    "# 3. Mostrar o gr√°fico\n",
    "print(\"Confusion Matrix (Original Model - No Tuning):\")\n",
    "disp_original.plot(cmap=plt.cm.Blues, colorbar=False)\n",
    "disp_original.ax_.grid(False) # Desligar a grelha\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd30c2e-e40e-4ce5-b540-eb248bb545e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√©lula 10: Imports e defini√ß√£o do Pipeline de Tuning\n",
    "# 1. Criar o pipeline COMPLETO\n",
    "# Junta o teu 'preprocessor' (que j√° tens) com o 'LogisticRegression'\n",
    "#\n",
    "# Nota: Estou a adicionar solver='liblinear' e max_iter=1000\n",
    "# 'liblinear' √© bom para datasets pequenos e lida bem com a regulariza√ß√£o L1/L2\n",
    "# 'max_iter' mais alto evita avisos de converg√™ncia chatos.\n",
    "\n",
    "pipe_lr = Pipeline([\n",
    "    ('pre', preprocessor), \n",
    "    ('model', LogisticRegression(solver='liblinear', max_iter=1000))\n",
    "])\n",
    "\n",
    "# 2. Definir a \"grelha\" de par√¢metros que queremos testar\n",
    "# Vamos testar a for√ßa da regulariza√ß√£o (C) e o tipo de regulariza√ß√£o (penalty)\n",
    "#\n",
    "# 'model__C' -> O '__' (duplo underscore) diz ao Pipeline: \n",
    "# \"Quero que passes o par√¢metro 'C' para o passo que chamei de 'model'\"\n",
    "\n",
    "param_grid = {\n",
    "    'model__penalty': ['l1', 'l2'],\n",
    "    'model__C': [0.001, 0.01, 0.1, 1, 10, 100] \n",
    "}\n",
    "\n",
    "# 3. Configurar o GridSearch\n",
    "# cv=5 -> 5-fold cross-validation (como j√° tinhas feito)\n",
    "# scoring='f1' -> Queremos otimizar para a pontua√ß√£o F1\n",
    "# n_jobs=-1 -> Usa todos os processadores do teu PC. Vai ser r√°pido.\n",
    "grid_lr = GridSearchCV(pipe_lr, param_grid, cv=5, scoring='f1', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd425700-b0e0-4a49-a7c0-5213fe7428e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√©lula 11: Executar o Tuning\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit no GridSearch (isto pode demorar uns segundos)\n",
    "# Ele vai usar X_train e y_train\n",
    "grid_lr.fit(X_train, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"GridSearch demorou {end_time - start_time:.2f} segundos.\")\n",
    "print(\"---\")\n",
    "print(f\"Melhor pontua√ß√£o F1 (em CV): {grid_lr.best_score_:.4f}\")\n",
    "print(\"Melhores par√¢metros encontrados:\")\n",
    "print(grid_lr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcc312b-0043-4a6d-a123-d9a06b7667c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√©lula 12: Avaliar o modelo otimizado no set de teste\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Pega no melhor estimador que o GridSearch encontrou\n",
    "best_lr = grid_lr.best_estimator_\n",
    "\n",
    "# Faz previs√µes no X_test\n",
    "y_pred_lr_tuned = best_lr.predict(X_test)\n",
    "\n",
    "print(\"Relat√≥rio de Classifica√ß√£o (Modelo Otimizado no Teste):\")\n",
    "print(classification_report(y_test, y_pred_lr_tuned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2948d0c9-1972-4f90-a0e0-ee6ba2c4cbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√©lula 13: An√°lise de Feature Importance\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Apanhar o modelo treinado (est√° dentro do pipeline)\n",
    "model = best_lr.named_steps['model']\n",
    "\n",
    "# 2. Apanhar o preprocessor (para sabermos os nomes das colunas)\n",
    "preprocessor = best_lr.named_steps['pre']\n",
    "\n",
    "# 3. Apanhar os nomes das features\n",
    "# O preprocessor tem 2 transformadores ('num' e 'cat')\n",
    "# O transformador 'cat' tem o 'onehot' l√° dentro\n",
    "onehot_features = preprocessor.transformers_[1][1].named_steps['onehot'].get_feature_names_out(cat_features)\n",
    "\n",
    "# Juntar os nomes das features num√©ricas + one-hot\n",
    "# (A ordem TEM de ser a mesma do ColumnTransformer: num, depois cat)\n",
    "all_features = num_features + list(onehot_features)\n",
    "\n",
    "# 4. Criar um DataFrame bonito para ver os resultados\n",
    "coefs = model.coef_[0]\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': all_features,\n",
    "    'Coeficiente': coefs,\n",
    "    'Abs_Coef': abs(coefs) # Valor absoluto para ordenar por import√¢ncia\n",
    "})\n",
    "\n",
    "# 5. Mostrar as 10 features mais impactantes (positivas ou negativas)\n",
    "print(\"Features mais importantes (impacto total):\")\n",
    "display(feature_importance.sort_values(by='Abs_Coef', ascending=False).head(10))\n",
    "\n",
    "# 6. Mostrar as 5 que mais levam ao \"Sim\" (treatment=1)\n",
    "print(\"\\nFeatures que mais indicam 'Treatment = Yes':\")\n",
    "display(feature_importance.sort_values(by='Coeficiente', ascending=False).head(5))\n",
    "\n",
    "# 7. Mostrar as 5 que mais levam ao \"N√£o\" (treatment=0)\n",
    "print(\"\\nFeatures que mais indicam 'Treatment = No':\")\n",
    "display(feature_importance.sort_values(by='Coeficiente', ascending=True).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3f98c5-0991-4c1d-b5e3-4e204734b5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Confusion Matrix (Clean Version)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# 1. Generate the matrix (this stays the same)\n",
    "cm = confusion_matrix(y_test, y_pred_lr_tuned)\n",
    "\n",
    "# 2. Draw the matrix (this stays the same)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                              display_labels=['No', 'Yes'])\n",
    "\n",
    "# 3. Show the plot (THIS IS WHERE WE CHANGE IT)\n",
    "print(\"Confusion Matrix (Optimized Model):\")\n",
    "\n",
    "# We add 'colorbar=False' to remove the scale on the right\n",
    "disp.plot(cmap=plt.cm.Blues, colorbar=False) \n",
    "\n",
    "# We access the plot's 'axis' (ax_) and turn off the grid\n",
    "disp.ax_.grid(False) # <-- The magic to remove the lines\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a0b1dd-06ce-40ab-bd14-a97bc34f17f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: ROC Curve (Receiver Operating Characteristic)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import RocCurveDisplay, roc_auc_score\n",
    "\n",
    "# 1. First, let's create the \"canvas\" (the axis)\n",
    "# This allows us to draw both lines on the same plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# 2. Now, we tell RocCurveDisplay to draw itself ON THAT canvas\n",
    "# It uses .predict_proba() automatically to get the \"confidence\"\n",
    "RocCurveDisplay.from_estimator(\n",
    "    best_lr, # Your tuned model from GridSearch\n",
    "    X_test,\n",
    "    y_test,\n",
    "    name='Logistic Regression (Tuned)', # Name for the legend\n",
    "    ax=ax # Tells it to use the canvas we created\n",
    ")\n",
    "\n",
    "# 3. Finally, we draw the \"chance\" (Dummy) line on the SAME canvas\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Chance (AUC = 0.50)') # 'k--' is a black dashed line\n",
    "    \n",
    "# 4. Clean up and show\n",
    "ax.set_title('ROC Curve (Optimized Model)')\n",
    "ax.legend() # Activates the legend (which shows your model's AUC)\n",
    "plt.show()\n",
    "\n",
    "# 5. (Optional) Print the AUC score separately\n",
    "# The plot already shows it, but this way you have the number\n",
    "y_probs = best_lr.predict_proba(X_test)[:, 1] # Probabilities for the \"Yes\" class only\n",
    "auc_score = roc_auc_score(y_test, y_probs)\n",
    "print(f\"Area Under the Curve (AUC Score): {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b2f599-38c4-4b16-b2bc-d70ef92a1594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√©lula 16: An√°lise dos Erros do Modelo\n",
    "# 1. Identificar quais previs√µes est√£o erradas\n",
    "errors_mask = (y_pred_lr_tuned != y_test)\n",
    "\n",
    "# 2. Ver as caracter√≠sticas das pessoas mal classificadas\n",
    "X_test_errors = X_test[errors_mask]\n",
    "y_test_errors = y_test[errors_mask]\n",
    "y_pred_errors = y_pred_lr_tuned[errors_mask]\n",
    "\n",
    "print(f\"Total de erros: {errors_mask.sum()} / {len(y_test)} ({errors_mask.sum()/len(y_test)*100:.1f}%)\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# 3. Separar os dois tipos de erro\n",
    "false_positives = (y_pred_errors == 1) & (y_test_errors == 0)\n",
    "false_negatives = (y_pred_errors == 0) & (y_test_errors == 1)\n",
    "\n",
    "print(f\"\\n‚ùå Falsos Positivos: {false_positives.sum()} casos\")\n",
    "print(\"   (Modelo disse 'Yes' mas era 'No')\")\n",
    "print(f\"\\n‚ùå Falsos Negativos: {false_negatives.sum()} casos\")\n",
    "print(\"   (Modelo disse 'No' mas era 'Yes')\")\n",
    "\n",
    "# 4. Analisar caracter√≠sticas dos falsos positivos\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä PERFIL DOS FALSOS POSITIVOS:\")\n",
    "print(\"=\"*60)\n",
    "if false_positives.sum() > 0:\n",
    "    fp_data = X_test_errors[false_positives]\n",
    "    print(\"\\nIdade m√©dia:\", fp_data['Age'].mean())\n",
    "    print(\"\\nG√©nero:\")\n",
    "    print(fp_data['Gender_clean'].value_counts())\n",
    "    print(\"\\nWork Interfere:\")\n",
    "    print(fp_data['work_interfere'].value_counts())\n",
    "\n",
    "# 5. Analisar caracter√≠sticas dos falsos negativos\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä PERFIL DOS FALSOS NEGATIVOS:\")\n",
    "print(\"=\"*60)\n",
    "if false_negatives.sum() > 0:\n",
    "    fn_data = X_test_errors[false_negatives]\n",
    "    print(\"\\nIdade m√©dia:\", fn_data['Age'].mean())\n",
    "    print(\"\\nG√©nero:\")\n",
    "    print(fn_data['Gender_clean'].value_counts())\n",
    "    print(\"\\nWork Interfere:\")\n",
    "    print(fn_data['work_interfere'].value_counts())\n",
    "\n",
    "# 6. VISUALIZA√á√ÉO: Comparar distribui√ß√µes\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Gr√°fico 1: Idade\n",
    "axes[0].hist(X_test[~errors_mask]['Age'].dropna(), bins=20, alpha=0.5, label='Corretos', color='green')\n",
    "axes[0].hist(X_test_errors['Age'].dropna(), bins=20, alpha=0.5, label='Erros', color='red')\n",
    "axes[0].set_xlabel('Idade')\n",
    "axes[0].set_ylabel('Frequ√™ncia')\n",
    "axes[0].set_title('Distribui√ß√£o de Idade: Corretos vs Erros')\n",
    "axes[0].legend()\n",
    "\n",
    "# Gr√°fico 2: Work Interfere\n",
    "work_counts_correct = X_test[~errors_mask]['work_interfere'].value_counts()\n",
    "work_counts_errors = X_test_errors['work_interfere'].value_counts()\n",
    "\n",
    "x = range(len(work_counts_correct))\n",
    "width = 0.35\n",
    "axes[1].bar([i - width/2 for i in x], work_counts_correct.values, width, label='Corretos', color='green', alpha=0.7)\n",
    "axes[1].bar([i + width/2 for i in x], work_counts_errors.values, width, label='Erros', color='red', alpha=0.7)\n",
    "axes[1].set_xlabel('Work Interfere')\n",
    "axes[1].set_ylabel('Frequ√™ncia')\n",
    "axes[1].set_title('Work Interfere: Corretos vs Erros')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(work_counts_correct.index, rotation=45)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5798a799-65b7-46c3-b62c-3862aec2ed37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
