{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc1fb25-6959-4ef0-8b27-070da6e8f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 — Imports (Machine Learning & Deep Learning)\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Scikit-Learn: classic machine learning tools\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "\n",
    "# Metrics (corrected and expanded)\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_auc_score,\n",
    "    RocCurveDisplay,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# TensorFlow / Keras: deep learning framework\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Display settings\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"All libraries imported successfully — TensorFlow/Keras and f1_score included!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893125a3-7d11-491a-878c-e4306ef95189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load CSV and get a quick overview\n",
    "df = pd.read_csv('survey.csv')\n",
    "\n",
    "# Show the dimensions of the dataset\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "\n",
    "# Display the first 6 rows\n",
    "display(df.head(6))\n",
    "\n",
    "# Display summary info about the dataset (column types, non-null counts, etc.)\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ef18e8-b8ee-4b54-b505-c7f92f80a8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Exploratory Data Analysis (EDA)\n",
    "\n",
    "# List all columns\n",
    "print(\"Columns in the dataset:\", list(df.columns))\n",
    "\n",
    "# Show counts of each value in the target column ('treatment')\n",
    "print(\"\\nTarget value counts (treatment):\")\n",
    "print(df['treatment'].value_counts(dropna=False))\n",
    "\n",
    "# Calculate percentage of missing values per column\n",
    "missing_pct = df.isna().mean().sort_values(ascending=False) * 100\n",
    "print(\"\\nColumns with missing values (%):\")\n",
    "display(missing_pct[missing_pct > 0].round(2))\n",
    "\n",
    "# Visualize the distribution of the target variable\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.countplot(data=df, x='treatment', order=df['treatment'].value_counts().index)\n",
    "plt.title('Distribution of the target: treatment')\n",
    "plt.show()\n",
    "\n",
    "# Visualize the distribution of age\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(df['Age'].dropna(), bins=30)\n",
    "plt.title('Age distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5391d1-6000-4c1e-b109-eb33775bb1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — Basic data cleaning: age validation, gender normalization, and Yes/No encoding\n",
    "\n",
    "df2 = df.copy()  # work on a copy to avoid modifying the original dataset\n",
    "\n",
    "# Clean and validate the Age column\n",
    "df2['Age'] = pd.to_numeric(df2['Age'], errors='coerce')  # convert to numeric; invalid values become NaN\n",
    "print(\"Before filtering: number of missing ages =\", df2['Age'].isna().sum())\n",
    "\n",
    "# Optionally remove unrealistic ages (outside 14–100)\n",
    "df2.loc[(df2['Age'] < 14) | (df2['Age'] > 100), 'Age'] = np.nan\n",
    "print(\"After filtering: number of missing ages =\", df2['Age'].isna().sum())\n",
    "\n",
    "\n",
    "# Normalize gender labels into three categories: Male, Female, Other\n",
    "def clean_gender(x):\n",
    "    if pd.isna(x):\n",
    "        return 'Other'\n",
    "    s = str(x).strip().lower()\n",
    "\n",
    "    # common male variations\n",
    "    if s in ['male', 'm', 'man', 'male-ish', 'maile', 'mal', 'cis male', 'male (cis)']:\n",
    "        return 'Male'\n",
    "    \n",
    "    # common female variations\n",
    "    if s in ['female', 'f', 'woman', 'female (cis)', 'cis female']:\n",
    "        return 'Female'\n",
    "    \n",
    "    # any other value (including trans, non-binary, etc.)\n",
    "    return 'Other'\n",
    "\n",
    "df2['Gender_clean'] = df2['Gender'].apply(clean_gender)\n",
    "\n",
    "\n",
    "# Encode Yes/No columns as 1/0\n",
    "binary_cols = ['self_employed', 'family_history', 'treatment', 'remote_work', 'tech_company']\n",
    "\n",
    "# Note: this logic is fragile and was the source of some issues.\n",
    "# A corrected version is applied later in Cell 7.\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "    for col in binary_cols:\n",
    "        if col in df2.columns:\n",
    "            df2[col] = df2[col].map({'Yes': 1, 'No': 0})   # map Yes/No to 1/0\n",
    "            df2[col] = df2[col].fillna(df2[col]).infer_objects(copy=False)\n",
    "\n",
    "print(\"Basic cleaning complete. Here's a sample:\")\n",
    "display(df2[['Age', 'Gender', 'Gender_clean', 'self_employed', 'family_history', 'treatment']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f4fc1f-588f-41d3-9da6-d2756c81fcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 — Initial feature selection and simple feature creation\n",
    "\n",
    "df3 = df2.copy()  # create a new copy so earlier cleaning steps remain intact\n",
    "\n",
    "# Example: a potential binary feature such as \"long_hours\" (e.g., working more than 50 hours per week)\n",
    "# Since the dataset does not include an \"hours\" column, this feature is not created here.\n",
    "\n",
    "# List of potential features to use in the analysis and modeling steps\n",
    "candidate_features = [\n",
    "    'Age',\n",
    "    'Gender_clean',\n",
    "    'self_employed',\n",
    "    'family_history',\n",
    "    'work_interfere',     # categorical: Never, Rarely, Sometimes, Often\n",
    "    'no_employees',       # categorical: company size\n",
    "    'remote_work',\n",
    "    'tech_company',\n",
    "    'benefits',\n",
    "    'care_options',\n",
    "    'wellness_program',\n",
    "    'seek_help',\n",
    "    'anonymity'\n",
    "]\n",
    "\n",
    "# Keep only the features that actually exist in the dataframe\n",
    "candidate_features = [c for c in candidate_features if c in df3.columns]\n",
    "print(\"Candidate features being used:\", candidate_features)\n",
    "\n",
    "# Quick look at unique values for categorical features or any feature with fewer than 20 unique values\n",
    "for col in candidate_features:\n",
    "    if df3[col].dtype == 'object' or df3[col].nunique() < 20:\n",
    "        print(f\"\\nUnique values in '{col}':\")\n",
    "        print(df3[col].fillna('NA').value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ca9683-185f-4d9b-94b6-4c92927dd790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 — Preprocessing pipeline\n",
    "# This is the cell that failed in notebook 1. Here we use the corrected version that includes FunctionTransformer.\n",
    "\n",
    "print(\"Defining the preprocessor (with the .astype(str) fix)...\")\n",
    "\n",
    "# Split candidate features into numeric and categorical sets\n",
    "num_features = [c for c in candidate_features\n",
    "                if df3[c].dtype in ['int64', 'float64'] and c != 'treatment']\n",
    "cat_features = [c for c in candidate_features if c not in num_features]\n",
    "\n",
    "print(\"Numerical features:\", num_features)\n",
    "print(\"Categorical features:\", cat_features)\n",
    "\n",
    "# Numerical pipeline: median imputation followed by standard scaling\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline: constant imputation, force to string, then one-hot encode\n",
    "# The FunctionTransformer with astype(str) avoids the TypeError caused by mixed dtypes.\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('to_string', FunctionTransformer(lambda x: x.astype(str))),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine numeric and categorical pipelines into a single ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_pipeline, num_features),\n",
    "        ('cat', cat_pipeline, cat_features)\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    sparse_threshold=0\n",
    ")\n",
    "\n",
    "print(\"Preprocessor (fixed) created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217b5679-1052-4583-845d-65771f25f3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Prepare X, y and create the Train/Test split (for K-Fold later)\n",
    "\n",
    "print(\"Preparing X and y (with the robust pd.to_numeric fix)...\")\n",
    "\n",
    "# Features\n",
    "X = df3[candidate_features].copy()\n",
    "\n",
    "# Robust target conversion (same original logic)\n",
    "y_temp = df3['treatment'].replace({'Yes': 1, 'No': 0})\n",
    "y = pd.to_numeric(y_temp, errors='coerce')\n",
    "\n",
    "# Remove rows where the target became NaN\n",
    "mask = y.notna()\n",
    "X = X[mask]\n",
    "y = y[mask].astype(int)\n",
    "\n",
    "print(f\"Shape after removing NaNs from target: X={X.shape}, y={y.shape}\")\n",
    "\n",
    "# Updated Logic: Create two sets (Train_Full / Test)\n",
    "\n",
    "# Create a true hold-out Test Set (20% of the data)\n",
    "# This data will only be used once, at the very end.\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# The Train_Full set (80% of the data) is what the K-Fold will use or its internal training/validation splits.\n",
    "\n",
    "print(\"\\nShapes of the two sets (80/20 split):\")\n",
    "print(f\"Train_Full: {X_train_full.shape}, {y_train_full.shape}  (used for K-Fold)\")\n",
    "print(f\"Test:       {X_test.shape}, {y_test.shape}            (held out until the end)\")\n",
    "\n",
    "print(\"\\nTrain_Full target distribution:\", np.bincount(y_train_full) / len(y_train_full))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e99debd-6295-4bc0-a146-f801ef3403c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Preprocess Data and Define the \"Raw\" Neural Network Functions (K-Fold Version)\n",
    "\n",
    "# Apply the preprocessor\n",
    "print(\"Applying the preprocessor (fit on Train_Full, transform on both sets)...\")\n",
    "\n",
    "# Fit on the full training set (80%)\n",
    "preprocessor.fit(X_train_full)\n",
    "\n",
    "# Transform both Train_Full and Test\n",
    "X_train_full_processed = preprocessor.transform(X_train_full)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Convert processed data to NumPy \"raw\" format (transposed)\n",
    "X_train_full_raw = X_train_full_processed.T\n",
    "X_test_raw = X_test_processed.T\n",
    "\n",
    "# Convert y to 1-row arrays\n",
    "y_train_full_raw = y_train_full.values.reshape(1, y_train_full.shape[0])\n",
    "y_test_raw = y_test.values.reshape(1, y_test.shape[0])\n",
    "\n",
    "print(f\"Raw format of X_train_full (features, samples): {X_train_full_raw.shape}\")\n",
    "print(f\"Raw format of X_test (features, samples):      {X_test_raw.shape}\")\n",
    "\n",
    "# Helper functions (same logic as before)\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Computes the sigmoid (logistic) activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_gradient(z):\n",
    "    \"\"\"Computes the derivative of the sigmoid (for backpropagation).\"\"\"\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0452911-2e5b-4dbf-b92f-e92efbebf3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Initialize Parameters\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Initializes the weight matrices (Thetas) with small random values.\n",
    "    \n",
    "    n_x: number of input neurons (your features)\n",
    "    n_h: number of neurons in the hidden layer\n",
    "    n_y: number of output neurons (in our case, this is 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(42)  # Ensures reproducibility\n",
    "    \n",
    "    # W1 is Theta_1 (input layer -> hidden layer)\n",
    "    # W2 is Theta_2 (hidden layer -> output layer)\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01   # Breaks symmetry\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    \n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c6d9ed-99dc-4275-b9b8-9c545ed50b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Forward Propagation\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a simple 2-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data of shape (n_x, m)\n",
    "    parameters -- dictionary containing network weights and biases\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- output of the sigmoid activation (predictions)\n",
    "    cache -- dictionary containing intermediate values for backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve weights and biases from the parameters dictionary\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    # Layer 1 (Hidden Layer): Linear combination followed by tanh activation\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)  # tanh activation for hidden layer\n",
    "    \n",
    "    # Layer 2 (Output Layer): Linear combination followed by sigmoid activation\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)  # sigmoid activation for binary classification\n",
    "    \n",
    "    # Store intermediate results in cache for use in backpropagation\n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b89905-111c-4aba-93b2-36c52f32757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Compute Cost Function (with L2 Regularization)\n",
    "def compute_cost(A2, Y, parameters, lambda_reg):\n",
    "    \"\"\"\n",
    "    Computes the total cost for a 2-layer neural network, \n",
    "    combining Binary Cross-Entropy loss with L2 regularization.\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- predictions from the output layer (sigmoid), shape (1, m)\n",
    "    Y -- true labels (0 or 1), shape (1, m)\n",
    "    parameters -- dictionary containing network weights W1 and W2\n",
    "    lambda_reg -- L2 regularization hyperparameter\n",
    "    \n",
    "    Returns:\n",
    "    total_cost -- scalar value representing the total cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]  # number of examples\n",
    "    \n",
    "    # Retrieve weights for regularization\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    \n",
    "    # Binary Cross-Entropy cost\n",
    "    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), 1 - Y)\n",
    "    cross_entropy_cost = - (1 / m) * np.sum(logprobs)\n",
    "    \n",
    "    # L2 regularization cost\n",
    "    L2_cost = (lambda_reg / (2 * m)) * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
    "    \n",
    "    # Total cost = data loss + regularization loss\n",
    "    total_cost = cross_entropy_cost + L2_cost\n",
    "    \n",
    "    # Ensure the cost is returned as a scalar\n",
    "    return np.squeeze(total_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00f2f28-e912-4c9f-af8d-3075623d8c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Backward Propagation\n",
    "def backward_propagation(parameters, cache, X, Y, lambda_reg):\n",
    "    \"\"\"\n",
    "    Implements the backpropagation algorithm for a 2-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- dictionary containing network weights W1 and W2\n",
    "    cache -- dictionary containing intermediate values from forward propagation\n",
    "    X -- input data, shape (n_x, m)\n",
    "    Y -- true labels, shape (1, m)\n",
    "    lambda_reg -- L2 regularization hyperparameter\n",
    "    \n",
    "    Returns:\n",
    "    grads -- dictionary containing gradients of weights and biases\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]  # number of examples\n",
    "    \n",
    "    # Retrieve weights\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    \n",
    "    # Retrieve cached values from forward propagation\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    Z1 = cache['Z1']\n",
    "    \n",
    "    # Compute the error at the output layer\n",
    "    dZ2 = A2 - Y  # derivative of cost w.r.t. Z2 (output pre-activation)\n",
    "    \n",
    "    # Gradients for weights and biases between hidden and output layer\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T) + (lambda_reg / m) * W2  # include L2 regularization\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    # Backpropagate the error to the hidden layer\n",
    "    # derivative of tanh activation: 1 - A1^2\n",
    "    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))\n",
    "    \n",
    "    # Gradients for weights and biases between input and hidden layer\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T) + (lambda_reg / m) * W1  # include L2 regularization\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    # Pack gradients into a dictionary\n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "    \n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d1d2f3-de24-4c7c-a4ac-5e5fbb223f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Update Parameters (Gradient Descent)\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Updates the weights and biases of the network using gradient descent.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- dictionary containing current weights and biases\n",
    "    grads -- dictionary containing gradients of weights and biases\n",
    "    learning_rate -- step size for gradient descent updates\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- dictionary containing updated weights and biases\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve current weights and biases\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    # Retrieve corresponding gradients\n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2']\n",
    "    \n",
    "    # Update rule for gradient descent\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    \n",
    "    # Pack updated parameters back into a dictionary\n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3921c4-ab74-42af-a8ea-f371ae8c3998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: The Full Neural Network Model\n",
    "def nn_model(X, Y, n_h, num_epochs, lambda_reg, learning_rate, print_cost=False):\n",
    "    \"\"\"\n",
    "    Builds and trains a simple 2-layer neural network from scratch.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, shape (n_x, m)\n",
    "    Y -- true labels, shape (1, m)\n",
    "    n_h -- number of neurons in the hidden layer\n",
    "    num_epochs -- number of training iterations\n",
    "    lambda_reg -- L2 regularization hyperparameter\n",
    "    learning_rate -- learning rate for gradient descent\n",
    "    print_cost -- if True, prints the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- learned weights and biases\n",
    "    costs -- list of costs recorded during training (every 100 epochs)\n",
    "    \"\"\"\n",
    "    \n",
    "    n_x = X.shape[0]  # number of input features\n",
    "    n_y = Y.shape[0]  # number of outputs (1 for binary classification)\n",
    "    costs = []\n",
    "    \n",
    "    # 1. Initialize parameters\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    # 2. Training loop\n",
    "    for i in range(num_epochs):\n",
    "        \n",
    "        # 3. Forward propagation\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # 4. Compute cost with L2 regularization\n",
    "        cost = compute_cost(A2, Y, parameters, lambda_reg)\n",
    "        \n",
    "        # 5. Backward propagation\n",
    "        grads = backward_propagation(parameters, cache, X, Y, lambda_reg)\n",
    "        \n",
    "        # 6. Update parameters using gradient descent\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Optionally print the cost every 100 epochs\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(f\"Cost after epoch {i}: {cost}\")\n",
    "            costs.append(cost)\n",
    "            \n",
    "    return parameters, costs\n",
    "\n",
    "\n",
    "# Function to make predictions with the trained model\n",
    "def predict(X, parameters):\n",
    "    \"\"\"\n",
    "    Uses the trained neural network to predict binary labels.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, shape (n_x, m)\n",
    "    parameters -- learned weights and biases\n",
    "    \n",
    "    Returns:\n",
    "    predictions -- predicted labels (0 or 1)\n",
    "    \"\"\"\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = (A2 > 0.5).astype(int)  # threshold at 0.5 for binary classification\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c223c78-6afd-48da-aa90-8babcdeede19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Train the \"Raw\" MLP (K-Fold Version)\n",
    "\n",
    "# Hyperparameters\n",
    "n_h = 8              # number of neurons in hidden layer\n",
    "num_epochs = 2000     # number of training iterations\n",
    "lambda_reg = 0.1      # L2 regularization parameter\n",
    "learning_rate = 0.01  # step size for gradient descent\n",
    "\n",
    "print(\"Starting training of the 'raw' model on the full training set...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the neural network on the full training set\n",
    "parameters, costs = nn_model(\n",
    "    X_train_full_raw, y_train_full_raw,  # use the complete training set\n",
    "    n_h=n_h, \n",
    "    num_epochs=num_epochs, \n",
    "    lambda_reg=lambda_reg, \n",
    "    learning_rate=learning_rate, \n",
    "    print_cost=True\n",
    ")\n",
    "\n",
    "print(f\"Training completed in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "# Plot the learning curve to visualize how the cost decreased over epochs\n",
    "plt.plot(np.squeeze(costs))\n",
    "plt.ylabel('Cost (Loss)')\n",
    "plt.xlabel('Epochs (x100)')\n",
    "plt.title(f\"Learning Curve (Learning Rate = {learning_rate})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ea029e-f661-439f-9a91-dfc44b45d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Evaluate the \"Raw\" MLP (K-Fold Version)\n",
    "\n",
    "# Make predictions on the TEST set\n",
    "y_pred_raw_test = predict(X_test_raw, parameters)\n",
    "\n",
    "# Make predictions on the full TRAIN set\n",
    "y_pred_raw_train = predict(X_train_full_raw, parameters)  # updated to full train set\n",
    "\n",
    "# Flatten predictions for compatibility with sklearn metrics\n",
    "y_pred_flat_test = y_pred_raw_test.flatten()\n",
    "y_pred_flat_train = y_pred_raw_train.flatten()\n",
    "\n",
    "# Performance Diagnostics\n",
    "print(\"\\n--- Performance Diagnostics ---\\n\")\n",
    "\n",
    "# Classification report on the TRAIN set\n",
    "print(\"Classification Report (TRAIN_FULL):\")\n",
    "print(classification_report(y_train_full, y_pred_flat_train, digits=4))\n",
    "print(\"---\")\n",
    "\n",
    "# Classification report on the TEST set\n",
    "print(\"Classification Report (TEST):\")\n",
    "print(classification_report(y_test, y_pred_flat_test, digits=4))\n",
    "print(\"---\")\n",
    "\n",
    "# Confusion Matrix for TEST set\n",
    "print(\"\\nConfusion Matrix (TEST):\\n\")\n",
    "cm = confusion_matrix(y_test, y_pred_flat_test)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No', 'Yes'])\n",
    "disp.plot(cmap=plt.cm.Blues, colorbar=False)\n",
    "disp.ax_.grid(False)  # remove grid for cleaner plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc57820-eb5e-401f-a3d0-deed68f58e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: ROC Curve for the \"Raw\" Model\n",
    "\n",
    "# Get the predicted Probabilitys for the test set\n",
    "# (We use forward_propagation to retrieve the output layer probabilities, A2)\n",
    "# 'parameters' comes from Cell 15\n",
    "# 'X_test_raw' comes from Cell 8\n",
    "probs_raw_test, _ = forward_propagation(X_test_raw, parameters)\n",
    "probs_raw_test = probs_raw_test.flatten()  # flatten for sklearn metrics\n",
    "\n",
    "# Compute the AUC score (Area Under the Curve)\n",
    "auc_raw_test = roc_auc_score(y_test, probs_raw_test)\n",
    "print(f\"AUC of the 'Raw' Model (Test Set): {auc_raw_test:.4f}\")\n",
    "\n",
    "# Plot the ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = plt.gca()  # get current axes\n",
    "\n",
    "RocCurveDisplay.from_predictions(\n",
    "    y_test, \n",
    "    probs_raw_test, \n",
    "    name=f\"Raw Model (AUC = {auc_raw_test:.3f})\", \n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# Plot the random classifier line for reference\n",
    "plt.plot([0, 1], [0, 1], 'r--', label='Random Classifier (AUC = 0.50)')\n",
    "\n",
    "plt.title('ROC Curve - \"Raw\" Model (Test Set)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccd28ef-56f3-4b08-b15b-35ba20dcd838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: The \"Refined\" Model (with Early Stopping)\n",
    "\n",
    "# This is a modified copy of 'nn_model' (Cell 14)\n",
    "# that includes Early Stopping.\n",
    "def nn_model_refined(X_train, Y_train, X_val, Y_val, n_h, num_epochs, lambda_reg, learning_rate, patience=20):\n",
    "    \"\"\"\n",
    "    Builds and trains a 2-layer neural network with Early Stopping.\n",
    "    \n",
    "    Arguments:\n",
    "    X_train, Y_train -- training data and labels\n",
    "    X_val, Y_val -- validation data and labels (for overfitting monitoring)\n",
    "    n_h -- number of neurons in the hidden layer\n",
    "    num_epochs -- maximum number of training iterations\n",
    "    lambda_reg -- L2 regularization hyperparameter\n",
    "    learning_rate -- step size for gradient descent\n",
    "    patience -- number of epochs to wait without improvement before stopping\n",
    "    \n",
    "    Returns:\n",
    "    best_params -- parameters (weights and biases) from the best epoch\n",
    "    costs_train -- list of training costs over epochs\n",
    "    costs_val -- list of validation costs over epochs\n",
    "    \"\"\"\n",
    "    \n",
    "    n_x = X_train.shape[0]  # number of input features\n",
    "    n_y = Y_train.shape[0]  # number of outputs\n",
    "    costs_train = []\n",
    "    costs_val = []  # store validation costs\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    best_cost = float('inf')  # best validation cost so far\n",
    "    patience_counter = 0\n",
    "    best_params = {}\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        \n",
    "        # Training step\n",
    "        A2_train, cache_train = forward_propagation(X_train, parameters)\n",
    "        cost_train = compute_cost(A2_train, Y_train, parameters, lambda_reg)\n",
    "        grads = backward_propagation(parameters, cache_train, X_train, Y_train, lambda_reg)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Validation step\n",
    "        A2_val, _ = forward_propagation(X_val, parameters)\n",
    "        cost_val = compute_cost(A2_val, Y_val, parameters, lambda_reg)\n",
    "        \n",
    "        costs_train.append(cost_train)\n",
    "        costs_val.append(cost_val)\n",
    "\n",
    "        # Early Stopping logic\n",
    "        if cost_val < best_cost:\n",
    "            best_cost = cost_val\n",
    "            best_params = parameters.copy()  # save the best parameters\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"--- Early stopping triggered at epoch {i} ---\")\n",
    "            break\n",
    "            \n",
    "    return best_params, costs_train, costs_val\n",
    "\n",
    "print(\"Function 'nn_model_refined' (with Early Stopping) defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8762a49d-8771-4639-8c4b-88e04624b916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: Grid Search with K-Fold Cross-Validation \n",
    "\n",
    "print(\"Starting Grid Search with K-Fold (K=5) for the refined model...\")\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_h': [8, 16],              # hidden layer sizes to test\n",
    "    'lambda_reg': [0.1, 0.5, 1.0], # L2 regularization values\n",
    "    'learning_rate': [0.01]      # learning rate(s) to test\n",
    "}\n",
    "\n",
    "# Initialize K-Fold cross-validation\n",
    "N_SPLITS = 5\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "best_score_f1 = -1\n",
    "best_hyperparams = {}\n",
    "# (final model parameters will be trained at the end on all data)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over hyperparameter combinations\n",
    "for n_h in param_grid['n_h']:\n",
    "    for lr in param_grid['learning_rate']:\n",
    "        for lam in param_grid['lambda_reg']:\n",
    "            \n",
    "            print(f\"\\nTesting: n_h={n_h}, lr={lr}, lambda={lam}\")\n",
    "            fold_scores = []  # store F1-scores for each fold\n",
    "\n",
    "            # K-Fold Cross-Validation\n",
    "            for fold, (train_index, val_index) in enumerate(skf.split(X_train_full, y_train_full)):\n",
    "                \n",
    "                # Select \"raw\" data based on fold indices\n",
    "                X_train_fold = X_train_full_raw[:, train_index]\n",
    "                y_train_fold = y_train_full_raw[:, train_index]\n",
    "                X_val_fold = X_train_full_raw[:, val_index]\n",
    "                y_val_fold = y_train_full_raw[:, val_index]\n",
    "                \n",
    "                # For scoring with sklearn, we need the normal labels\n",
    "                y_val_fold_series = y_train_full.iloc[val_index]\n",
    "\n",
    "                # Train the refined model (with Early Stopping)\n",
    "                params, _, _ = nn_model_refined(\n",
    "                    X_train_fold, y_train_fold,\n",
    "                    X_val_fold, y_val_fold,  # validation fold\n",
    "                    n_h=n_h, \n",
    "                    num_epochs=2000,\n",
    "                    lambda_reg=lam, \n",
    "                    learning_rate=lr,\n",
    "                    patience=50\n",
    "                )\n",
    "                \n",
    "                # Evaluate this fold\n",
    "                y_pred_val_fold = predict(X_val_fold, params).flatten()\n",
    "                score = f1_score(y_val_fold_series, y_pred_val_fold)\n",
    "                fold_scores.append(score)\n",
    "            \n",
    "            # Compute average F1-score across folds\n",
    "            avg_f1_score = np.mean(fold_scores)\n",
    "            print(f\"-> Mean F1 (K-Fold): {avg_f1_score:.4f}  (Scores: {[round(s, 2) for s in fold_scores]})\")\n",
    "            \n",
    "            # Update best hyperparameters if this combination is better\n",
    "            if avg_f1_score > best_score_f1:\n",
    "                best_score_f1 = avg_f1_score\n",
    "                best_hyperparams = {'n_h': n_h, 'lr': lr, 'lambda': lam}\n",
    "\n",
    "print(f\"\\nGrid Search completed in {time.time() - start_time:.2f} seconds.\")\n",
    "print(f\"\\n--- BEST HYPERPARAMETERS (K-Fold) ---\")\n",
    "print(f\"Best average F1-score: {best_score_f1:.4f}\")\n",
    "print(f\"Best Hyperparameters: {best_hyperparams}\")\n",
    "\n",
    "# Final Retraining\n",
    "# Train the final model on all training data (80% of the dataset)\n",
    "# using the simple nn_model (Cell 14)\n",
    "print(\"\\nRetraining the final model on ALL training data...\")\n",
    "best_model_params, _ = nn_model(\n",
    "    X_train_full_raw, y_train_full_raw,\n",
    "    n_h=best_hyperparams['n_h'],\n",
    "    num_epochs=2000,  # fixed number of epochs\n",
    "    lambda_reg=best_hyperparams['lambda'],\n",
    "    learning_rate=best_hyperparams['lr'],\n",
    "    print_cost=False  # no printing\n",
    ")\n",
    "print(\"Final model retrained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0410d7b3-65a5-424a-977b-bfbd70a8502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: Final Evaluation of the Refined Model (Post K-Fold)\n",
    "\n",
    "# Make predictions on the test and train_full sets with the best model\n",
    "y_pred_refined_test = predict(X_test_raw, best_model_params).flatten()\n",
    "y_pred_refined_train = predict(X_train_full_raw, best_model_params).flatten()\n",
    "\n",
    "print(\"\\n--- Performance Diagnostics (REFINED MODEL POST K-FOLD) ---\")\n",
    "\n",
    "# Classification report for the training set\n",
    "print(\"Classification Report (TRAIN_FULL - Refined):\")\n",
    "print(classification_report(y_train_full, y_pred_refined_train, digits=4))\n",
    "print(\"---\")\n",
    "\n",
    "# Classification report for the test set\n",
    "print(\"Classification Report (TEST - Refined):\")\n",
    "print(classification_report(y_test, y_pred_refined_test, digits=4))\n",
    "print(\"---\")\n",
    "\n",
    "# Confusion matrix for the test set\n",
    "print(\"\\nConfusion Matrix (TEST - Refined):\\n\")\n",
    "cm = confusion_matrix(y_test, y_pred_refined_test)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No', 'Yes'])\n",
    "disp.plot(cmap=plt.cm.Blues, colorbar=False)\n",
    "disp.ax_.grid(False)  # remove grid for cleaner plot\n",
    "plt.show()\n",
    "\n",
    "# Visualization: Distribution of predicted probabilities\n",
    "print(\"\\nDistribution of Predicted Probabilities (TEST - Refined):\\n\")\n",
    "probs_refined, _ = forward_propagation(X_test_raw, best_model_params)\n",
    "probs_flat = probs_refined.flatten()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(probs_flat[y_test == 0], bins=30, alpha=0.5, label='Class 0 (No)')\n",
    "plt.hist(probs_flat[y_test == 1], bins=30, alpha=0.5, label='Class 1 (Yes)')\n",
    "plt.xlabel('Predicted Probability of \"Yes\"')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Predicted Probability Distribution (Refined Model)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Error Analysis\n",
    "print(\"\\n--- Error Analysis (TEST - Refined) ---\")\n",
    "misclassified_idx = np.where(y_test != y_pred_refined_test)[0]\n",
    "print(f\"Total errors: {len(misclassified_idx)} out of {len(y_test)}\")\n",
    "print(\"Showing the first 5 misclassified examples:\")\n",
    "\n",
    "for idx in misclassified_idx[:5]:\n",
    "    original_idx = y_test.index[idx]\n",
    "    true_label = y_test.iloc[idx]\n",
    "    pred_label = y_pred_refined_test[idx]\n",
    "    \n",
    "    print(f\"\\n* Original Index: {original_idx}\")\n",
    "    print(f\"  True Label: {true_label}, Predicted: {pred_label}\")\n",
    "    print(f\"  Features: {X.loc[original_idx].to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9129bd31-04d2-45b2-aa08-bdfa8cb2d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21: ROC Curve (Refined Model - Test Set)\n",
    "\n",
    "# Get predicted probabilities for the test set\n",
    "# (We use 'best_model_params' obtained from Cell 18)\n",
    "probs_refined_test, _ = forward_propagation(X_test_raw, best_model_params)\n",
    "probs_refined_test = probs_refined_test.flatten()  # flatten for sklearn metrics\n",
    "\n",
    "# Compute the AUC score (Area Under the Curve)\n",
    "auc_refined_test = roc_auc_score(y_test, probs_refined_test)\n",
    "print(f\"AUC of the Refined Model (Test Set): {auc_refined_test:.4f}\")\n",
    "\n",
    "# Plot the ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = plt.gca()  # get current axes\n",
    "\n",
    "RocCurveDisplay.from_predictions(\n",
    "    y_test, \n",
    "    probs_refined_test, \n",
    "    name=f\"Refined Model (AUC = {auc_refined_test:.3f})\", \n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# Plot the reference line for a random classifier\n",
    "plt.plot([0, 1], [0, 1], 'r--', label='Random Classifier (AUC = 0.50)')\n",
    "\n",
    "plt.title('ROC Curve - Refined Model Post-KFold (Test Set)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e8c6cf-f86f-4857-b214-7d5407d34c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22: Feature Importance Analysis (Refined Model)\n",
    "\n",
    "print(\"--- Feature Importance Analysis (Weights from Layer 1 of the Refined Model) ---\")\n",
    "\n",
    "# Retrieve the feature names\n",
    "# Instead of relying on another cell, we fetch them directly from the preprocessor\n",
    "# 'preprocessor', 'num_features', 'cat_features' come from Cell 6\n",
    "print(\"Fetching feature names from the preprocessor...\")\n",
    "onehot_features = preprocessor.transformers_[1][1].named_steps['onehot'].get_feature_names_out(cat_features)\n",
    "all_features = num_features + list(onehot_features)\n",
    "print(f\"Found {len(all_features)} features in total.\")\n",
    "\n",
    "# Get the weight matrix W1 from the best model\n",
    "# 'best_model_params' was obtained in Cell 18\n",
    "W1_refined = best_model_params['W1']\n",
    "\n",
    "# Compute feature importance\n",
    "# Here we take the mean of the absolute values of the weights for each input feature\n",
    "feature_importance_refined = np.mean(np.abs(W1_refined), axis=0)\n",
    "\n",
    "# Step 4: Create a DataFrame for easy visualization\n",
    "df_importance_refined = pd.DataFrame({\n",
    "    'Feature': all_features,\n",
    "    'Importance (Mean Abs W1)': feature_importance_refined\n",
    "})\n",
    "\n",
    "# Show the Top 10 most important features\n",
    "print(\"\\nTop 10 Features (based on mean absolute weights from the first layer):\")\n",
    "display(df_importance_refined.sort_values(by='Importance (Mean Abs W1)', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f701927-68c8-4181-92d1-d73c31c563ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 23: Prepare Data for Keras/TensorFlow Model\n",
    "\n",
    "print(\"--- Section: Keras/TensorFlow Model ---\")\n",
    "\n",
    "# Unlike our \"raw\" model, Keras/TensorFlow expects data in shape (n_samples, n_features)\n",
    "# We'll use the processed datasets from Cell 8 (before transposing).\n",
    "\n",
    "# Reminder of available datasets:\n",
    "# X_train_full_processed -> 80% of data, preprocessed features\n",
    "# y_train_full           -> 80% of data, target labels\n",
    "# X_test_processed       -> 20% of data, preprocessed features\n",
    "# y_test                 -> 20% of data, target labels\n",
    "\n",
    "# Create a validation split for Early Stopping in Keras\n",
    "# We'll use 20% of the training data as a validation set\n",
    "X_train_k, X_val_k, y_train_k, y_val_k = train_test_split(\n",
    "    X_train_full_processed, \n",
    "    y_train_full,\n",
    "    test_size=0.20,       # 20% of the 80% training data = 16% of total dataset\n",
    "    stratify=y_train_full, # keep the class distribution consistent\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define the number of input features (for the first layer of Keras)\n",
    "n_features = X_train_k.shape[1]\n",
    "\n",
    "# Display dataset shapes\n",
    "print(f\"Number of input features (input_shape): {n_features}\")\n",
    "print(f\"Training set shape: {X_train_k.shape}\")\n",
    "print(f\"Validation set shape: {X_val_k.shape}\")\n",
    "print(f\"Test set shape: {X_test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eda126c-bbc0-4b8b-8715-7c7b1442741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 24: Build the Keras Model (Deeper Architecture)\n",
    "\n",
    "# Retrieve the best L2 regularization value (lambda) found via K-Fold (Cell 18)\n",
    "# If 'best_hyperparams' doesn't exist, fall back to 0.1 as in Cell 15\n",
    "try:\n",
    "    best_lambda = best_hyperparams['lambda']\n",
    "except NameError:\n",
    "    best_lambda = 0.1 \n",
    "\n",
    "print(f\"Using L2 regularization (lambda) = {best_lambda}\")\n",
    "\n",
    "# Define the Sequential model\n",
    "model_keras = Sequential()\n",
    "\n",
    "# Input layer + first hidden layer\n",
    "# Use 'tanh' activation to match the \"raw\" model\n",
    "model_keras.add(Dense(\n",
    "    16,  # 16 neurons\n",
    "    activation='tanh',\n",
    "    kernel_regularizer=tf.keras.regularizers.l2(best_lambda),  # L2 regularization\n",
    "    input_shape=(n_features,)  # only needed for the first layer\n",
    "))\n",
    "\n",
    "# Second hidden layer\n",
    "model_keras.add(Dense(\n",
    "    8,   # 8 neurons\n",
    "    activation='tanh',\n",
    "    kernel_regularizer=tf.keras.regularizers.l2(best_lambda)\n",
    "))\n",
    "\n",
    "# Output layer\n",
    "model_keras.add(Dense(\n",
    "    1,   # single neuron (binary classification)\n",
    "    activation='sigmoid'  # sigmoid for probability output (0 to 1)\n",
    "))\n",
    "\n",
    "# Display the model architecture\n",
    "print(\"\\n--- Keras Model Architecture ---\")\n",
    "model_keras.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c2d6a6-5c88-4bc6-9ce2-4115bb7ca7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 25: Compile and Train the Keras Model\n",
    "\n",
    "# Define the Early Stopping callback\n",
    "# This automatically stops training if validation loss stops improving\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',          # Stop if the validation loss stops improving\n",
    "    patience=50,                 # Wait 50 epochs without improvement\n",
    "    restore_best_weights=True    # Restore the weights from the best epoch\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "model_keras.compile(\n",
    "    optimizer='adam',                   # Modern optimizer (better than plain gradient descent)\n",
    "    loss='binary_crossentropy',         # Loss function for binary classification\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]  # Metrics to track\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"\\n--- Starting Keras training (with progress bars!) ---\")\n",
    "\n",
    "# Set a high number of epochs (1000), but EarlyStopping will stop training early\n",
    "history = model_keras.fit(\n",
    "    X_train_k, y_train_k,\n",
    "    epochs=1000,                       # maximum epochs\n",
    "    batch_size=32,                     # mini-batch gradient descent\n",
    "    validation_data=(X_val_k, y_val_k), # validation set for EarlyStopping\n",
    "    callbacks=[early_stopping],         # pass the EarlyStopping callback\n",
    "    verbose=1                           # 1 = show progress bars\n",
    ")\n",
    "\n",
    "print(\"\\nKeras training completed (stopped early by EarlyStopping).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec772ca-d80e-47cf-a1ed-963bf5fd5328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 26: Final Evaluation of the Keras Model\n",
    "\n",
    "print(\"\\n--- Keras Model Evaluation (on Test Set) ---\")\n",
    "\n",
    "# Evaluate the model (Loss and Metrics)\n",
    "loss, accuracy, auc = model_keras.evaluate(X_test_processed, y_test, verbose=0)\n",
    "print(f\"Test Loss:     {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"Test AUC:      {auc:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_probs_k = model_keras.predict(X_test_processed)\n",
    "y_pred_k = (y_pred_probs_k > 0.5).astype(int).flatten()  # Round probabilities to 0 or 1\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report (TEST - Keras):\")\n",
    "print(classification_report(y_test, y_pred_k, digits=4))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\nConfusion Matrix (TEST - Keras):\")\n",
    "cm_k = confusion_matrix(y_test, y_pred_k)\n",
    "disp_k = ConfusionMatrixDisplay(confusion_matrix=cm_k, display_labels=['No', 'Yes'])\n",
    "disp_k.plot(cmap=plt.cm.Blues, colorbar=False)\n",
    "disp_k.ax_.grid(False)  # Remove grid for a cleaner display\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e5ed40-d953-4689-bd4e-433c331000cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 27: ROC Curve (Keras Model - Test Set)\n",
    "\n",
    "# Get predicted PROBABILITIES for the test set\n",
    "# Using model.predict() from Keras\n",
    "probs_keras = model_keras.predict(X_test_processed).flatten()  # flatten for sklearn metrics\n",
    "\n",
    "# Compute the AUC score (Area Under the Curve)\n",
    "auc_keras = roc_auc_score(y_test, probs_keras)\n",
    "print(f\"AUC of the Keras Model (Test Set): {auc_keras:.4f}\")\n",
    "\n",
    "# Plot the ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = plt.gca()  # get current axes\n",
    "\n",
    "RocCurveDisplay.from_predictions(\n",
    "    y_test,\n",
    "    probs_keras,\n",
    "    name=f\"Keras Model (AUC = {auc_keras:.3f})\",\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# Plot the reference line for a random classifier\n",
    "plt.plot([0, 1], [0, 1], 'r--', label='Random Classifier (AUC = 0.50)')\n",
    "\n",
    "plt.title('ROC Curve - Keras Model (Test Set)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028ac179-8c9e-4d06-845a-82e8c82bea89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
