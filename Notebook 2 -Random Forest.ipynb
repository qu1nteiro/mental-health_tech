{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60f434c-8728-477b-a86c-a34ab3dd5fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell One: imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, RocCurveDisplay\n",
    "\n",
    "# display options\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 200)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"Imports ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfafeb91-ad4f-4f32-b8fb-399da1a2c6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell two: load and view CSV  \n",
    "df = pd.read_csv('survey.csv')\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head(6))\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa801474-b7f4-4208-9fdd-a1121dbcaa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell three:Exploratory Data Analysis\n",
    "print(\"Columns:\", list(df.columns))\n",
    "print(\"\\nTarget value counts (treatment):\")\n",
    "print(df['treatment'].value_counts(dropna=False))\n",
    "\n",
    "# Missing values %\n",
    "missing_pct = df.isna().mean().sort_values(ascending=False) * 100\n",
    "display(missing_pct[missing_pct>0].round(2))\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.countplot(data=df, x='treatment', order=df['treatment'].value_counts().index)\n",
    "plt.title('Distribution of target: treatment')\n",
    "plt.show()\n",
    "\n",
    "# Age\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "# 1. Outliers removal\n",
    "df2 = df[(df['Age'] >= 14) & (df['Age'] <= 100)].copy()\n",
    "\n",
    "# 2. Usamos esse novo DataFrame para o gráfico\n",
    "sns.histplot(df2['Age'].dropna(), bins=30)\n",
    "plt.title('Age distribution (Filtered 0-100 years)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5899d36-9308-4a73-a06b-a56c1e2e761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 (THE REAL, ALL-IN-ONE PREP CELL - v3 - THE FINAL ONE)\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import FunctionTransformer # <-- VAMOS PRECISAR DISTO\n",
    "\n",
    "# --- 1. Define Cleaning Function ---\n",
    "def clean_gender(x):\n",
    "    if pd.isna(x): return 'Other'\n",
    "    s = str(x).strip().lower()\n",
    "    if s in ['male', 'm', 'man', 'male-ish', 'maile', 'mal', 'cis male', 'male (cis)']: return 'Male'\n",
    "    if s in ['female', 'f', 'woman', 'female (cis)', 'cis female']: return 'Female'\n",
    "    return 'Other'\n",
    "\n",
    "# --- 2. Copy df2 and APPLY ALL CLEANING ---\n",
    "df3 = df2.copy()\n",
    "\n",
    "# Clean Gender\n",
    "df3['Gender_clean'] = df3['Gender'].apply(clean_gender)\n",
    "\n",
    "# Clean binary columns\n",
    "bin_cols = ['self_employed','family_history','treatment','remote_work','tech_company',\n",
    "            'benefits', 'care_options', 'wellness_program', 'seek_help', 'anonymity']\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    for c in bin_cols:\n",
    "        if c in df3.columns:\n",
    "            series = df3[c].replace({'Yes':1, 'No':0})\n",
    "            df3.loc[:, c] = pd.to_numeric(series, errors='coerce')\n",
    "print(\"df3 cleaned (Gender and binary columns).\")\n",
    "\n",
    "# --- 3. Define Features ---\n",
    "candidate_features = [\n",
    "    'Age', 'Gender_clean', 'self_employed', 'family_history',\n",
    "    'work_interfere', 'no_employees', 'remote_work', 'tech_company',\n",
    "    'benefits', 'care_options', 'wellness_program', 'seek_help', 'anonymity'\n",
    "]\n",
    "candidate_features = [c for c in candidate_features if c in df3.columns]\n",
    "print(\"Candidate features selected.\")\n",
    "\n",
    "# --- 4. Define Preprocessor (COM A CORREÇÃO DO 'TypeError') ---\n",
    "num_features = [c for c in candidate_features if df3[c].dtype in ['int64','float64'] and c!='treatment']\n",
    "cat_features = [c for c in candidate_features if c not in num_features]\n",
    "print(f\"Num features: {num_features}\")\n",
    "print(f\"Cat features: {cat_features}\")\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# --- AQUI ESTÁ A CORREÇÃO ---\n",
    "# Vamos forçar tudo o que entra aqui a ser uma string\n",
    "# O Imputer corre, depois convertemos tudo para string, depois o OneHotEncoder corre.\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('to_string', FunctionTransformer(lambda x: x.astype(str))), # <-- O ERRO MORRE AQUI\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "# -----------------------------\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_features),\n",
    "    ('cat', cat_pipeline, cat_features)\n",
    "], remainder='drop', sparse_threshold=0)\n",
    "print(\"Preprocessor created.\")\n",
    "\n",
    "# --- 5. Define X and y (WITH ALL FIXES) ---\n",
    "target_col = 'treatment'\n",
    "print(f\"Shape before dropping target NaNs: {df3.shape}\")\n",
    "df3 = df3.dropna(subset=[target_col])\n",
    "print(f\"Shape after dropping target NaNs: {df3.shape}\")\n",
    "\n",
    "X = df3[candidate_features] \n",
    "y = df3[target_col] # Isto já é 1.0 / 0.0 da limpeza que fizemos em cima\n",
    "print(\"Target column 'y' is now numeric.\")\n",
    "\n",
    "# --- 6. Split the data ---\n",
    "STATE = 42 \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=STATE, \n",
    "    stratify=y\n",
    ")\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "\n",
    "# --- 7. Build the RF Pipeline ---\n",
    "pipe_rf = Pipeline([\n",
    "    ('pre', preprocessor), \n",
    "    ('model', RandomForestClassifier(\n",
    "        random_state=STATE,\n",
    "        class_weight='balanced'\n",
    "    ))\n",
    "])\n",
    "print(\"Random Forest pipeline created.\")\n",
    "print(\"--- ALL PREP IS DONE. YOU ARE CLEAR TO RUN CELL 8 (agora é a Cell 5). ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7134afbc-ce94-47e0-ba97-e07f1198f3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 (THE NEW, FINAL, ATOMIC BOMB CELL - Replaces your old GridSearch cell)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- ESTA PARTE JÁ CORREU, MAS NÃO FAZ MAL CORRER DE NOVO ---\n",
    "print(\"--- FORCING DATA CLEANUP ---\")\n",
    "print(f\"Data type in y_train BEFORE fix: {y_train.dtype}\")\n",
    "print(\"Value counts in y_train BEFORE fix:\")\n",
    "print(y_train.value_counts(dropna=False)) \n",
    "\n",
    "y_temp_numeric = pd.to_numeric(y_train.replace({'Yes': 1, 'No': 0}), errors='coerce')\n",
    "mask = y_temp_numeric.notna()\n",
    "X_train_clean = X_train[mask]\n",
    "y_train_clean = y_temp_numeric[mask]\n",
    "\n",
    "print(\"\\n--- DATA IS NOW CLEAN ---\")\n",
    "print(\"Value counts in y_train AFTER fix:\")\n",
    "print(y_train_clean.value_counts(dropna=False)) \n",
    "print(f\"Shape of X_train (clean): {X_train_clean.shape}\")\n",
    "print(f\"Shape of y_train (clean): {y_train_clean.shape}\")\n",
    "\n",
    "# --- 2. NOW WE RUN THE GRIDSEARCH ON THE CLEAN DATA ---\n",
    "param_grid_rf = {\n",
    "    'model__n_estimators': [100, 150],       # Number of trees\n",
    "    'model__max_depth': [10, 15, None],      # How deep trees can go (None = full)\n",
    "}\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=STATE)\n",
    "grid_rf = GridSearchCV(\n",
    "    pipe_rf, \n",
    "    param_grid_rf, \n",
    "    cv=cv, \n",
    "    scoring='f1', \n",
    "    n_jobs=4     # Using 4 cores\n",
    ")\n",
    "\n",
    "print(\"\\nStarting GridSearch on CLEAN data... this may take a minute.\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "grid_rf.fit(X_train_clean, y_train_clean) \n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"GridSearch took {end_time - start_time:.2f} seconds.\")\n",
    "print(\"---\")\n",
    "\n",
    "# --- A CORREÇÃO ESTÁ AQUI ---\n",
    "print(f\"Best F1 score (in CV): {grid_rf.best_score_:.4f}\")  # <-- Adiciona o underscore\n",
    "print(\"Best parameters found:\")\n",
    "print(grid_rf.best_params_) # <-- Adiciona o underscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf49376-24aa-4900-bd21-e387e0d05f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 (Old Cell 9): Evaluate on Test Set\n",
    "\n",
    "# Get the best RF model found by the GridSearch\n",
    "best_rf = grid_rf.best_estimator_\n",
    "\n",
    "# Make predictions on the (dirty) X_test.\n",
    "# O pipeline dentro do best_rf limpa-o automaticamente.\n",
    "# y_pred_rf will be clean (1.0 / 0.0)\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "\n",
    "# --- A CORREÇÃO FINAL ESTÁ AQUI ---\n",
    "# A variável y_test na memória está \"suja\" (tem 'Yes'/'No').\n",
    "# Temos de a limpar *exatamente* como limpámos a y_train.\n",
    "print(f\"y_test values BEFORE fix: {y_test.value_counts(dropna=False).index[0:2]}...\")\n",
    "\n",
    "# 1. Converte 'Yes'/'No' para 1/0 e \"Don't Know\" (ou outro texto) para NaN\n",
    "y_test_numeric = pd.to_numeric(y_test.replace({'Yes': 1, 'No': 0}), errors='coerce')\n",
    "\n",
    "# 2. Cria uma máscara de todas as linhas válidas (que NÃO são NaN)\n",
    "mask = y_test_numeric.notna()\n",
    "\n",
    "# 3. Aplica essa máscara a AMBOS, o y_test e o y_pred\n",
    "# Isto remove as linhas com um y_test inválido e mantém-nos alinhados\n",
    "y_test_clean = y_test_numeric[mask]\n",
    "y_pred_rf_clean = y_pred_rf[mask] # <-- Importante!\n",
    "\n",
    "print(f\"y_test values AFTER fix: {y_test_clean.value_counts(dropna=False).index[0:2]}...\")\n",
    "\n",
    "# Agora comparamos duas listas limpas.\n",
    "print(\"\\nRandom Forest Test Classification Report:\")\n",
    "print(classification_report(y_test_clean, y_pred_rf_clean, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aafc11-9aa6-49e7-b60d-1c5220faaba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: EXPERIMENT 2 - Define Preprocessor (LR-Selected Features)\n",
    "\n",
    "# --- 1. Define a NOVA lista de features ---\n",
    "# Baseado na tua Célula 12 do notebook 1, estas eram as features\n",
    "# que o L1 NÃO matou (i.e., com coeficiente != 0)\n",
    "# (Estou a excluir 'no_employees' e 'tech_company' que não apareceram no teu top 10)\n",
    "lr_features = [\n",
    "    'Age', \n",
    "    'Gender_clean', \n",
    "    'family_history', \n",
    "    'work_interfere', \n",
    "    'benefits', \n",
    "    'care_options', \n",
    "    'anonymity'\n",
    "]\n",
    "\n",
    "print(f\"Running Experiment 2 with {len(lr_features)} selected features.\")\n",
    "\n",
    "# --- 2. Separar as listas de features (num/cat) ---\n",
    "num_features_lr = [c for c in lr_features if c in num_features] # Re-usa as 'num_features' originais\n",
    "cat_features_lr = [c for c in lr_features if c in cat_features] # Re-usa as 'cat_features' originais\n",
    "\n",
    "print(f\"LR Num features: {num_features_lr}\")\n",
    "print(f\"LR Cat features: {cat_features_lr}\")\n",
    "\n",
    "# --- 3. Criar os NOVOS pipelines (com nomes _lr) ---\n",
    "# Usamos os mesmos \"steps\" de antes, incluindo o fix do .astype(str)\n",
    "\n",
    "num_pipeline_lr = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline_lr = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('to_string', FunctionTransformer(lambda x: x.astype(str))), # O nosso fix de TypeError\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# --- 4. Criar o NOVO Preprocessor ---\n",
    "preprocessor_lr = ColumnTransformer([\n",
    "    ('num', num_pipeline_lr, num_features_lr),\n",
    "    ('cat', cat_pipeline_lr, cat_features_lr)\n",
    "], remainder='drop', sparse_threshold=0) # 'remainder=drop' é automático\n",
    "\n",
    "print(\"New 'preprocessor_lr' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae63cd3f-fd79-4c8a-86ac-19847a43a8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: EXPERIMENT 2 - Run GridSearch (LR-Selected Features)\n",
    "\n",
    "# --- 1. Criar o NOVO pipeline de RF ---\n",
    "pipe_rf_lr = Pipeline([\n",
    "    ('pre', preprocessor_lr), # <-- Usa o NOVO preprocessor\n",
    "    ('model', RandomForestClassifier(\n",
    "        random_state=STATE,\n",
    "        class_weight='balanced'\n",
    "    ))\n",
    "])\n",
    "print(\"New 'pipe_rf_lr' created.\")\n",
    "\n",
    "# --- 2. Definir a Grelha de Parâmetros (pode ser a mesma) ---\n",
    "param_grid_rf = {\n",
    "    'model__n_estimators': [100, 150],\n",
    "    'model__max_depth': [10, 15, None],\n",
    "}\n",
    "\n",
    "# --- 3. Configurar o NOVO GridSearch ---\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=STATE)\n",
    "grid_rf_lr = GridSearchCV( # <-- NOVO nome de variável\n",
    "    pipe_rf_lr, \n",
    "    param_grid_rf, \n",
    "    cv=cv, \n",
    "    scoring='f1', \n",
    "    n_jobs=4 # Usar 4 cores, como antes\n",
    ")\n",
    "\n",
    "print(\"\\nStarting NEW GridSearch (LR features)...\")\n",
    "start_time_lr = time.time()\n",
    "\n",
    "# --- 4. Treinar com os DADOS DE TREINO CERTOS ---\n",
    "# A célula 4 (Super-Cell) criou o X_train (DataFrame)\n",
    "# A célula 5 (Bomba Atómica) criou o y_train_clean (Series numérico)\n",
    "# O nosso novo pipeline sabe selecionar as colunas do X_train.\n",
    "grid_rf_lr.fit(X_train, y_train_clean) # <-- Fit no X_train original (DataFrame)\n",
    "\n",
    "end_time_lr = time.time()\n",
    "print(f\"GridSearch #2 took {end_time_lr - start_time_lr:.2f} seconds.\")\n",
    "print(\"---\")\n",
    "print(f\"Best F1 score (in CV) [LR Features]: {grid_rf_lr.best_score_:.4f}\")\n",
    "print(\"Best parameters found [LR Features]:\")\n",
    "print(grid_rf_lr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a824b62-a2eb-46d4-820c-a4349eac820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: EXPERIMENT 2 - Final Evaluation\n",
    "print(\"--- Results for RF Model with LR-Selected Features ---\")\n",
    "\n",
    "# 1. Pega no melhor modelo\n",
    "best_rf_lr = grid_rf_lr.best_estimator_\n",
    "\n",
    "# 2. Faz previsões (no X_test original)\n",
    "y_pred_rf_lr = best_rf_lr.predict(X_test)\n",
    "\n",
    "# 3. Usa os DADOS DE TESTE LIMPOS (da célula 6 antiga)\n",
    "# (y_test_clean e y_pred_rf_clean [só para a máscara])\n",
    "# (Temos de refazer a limpeza do y_test, já que a Célula 6 antiga pode não ter corrido)\n",
    "y_test_numeric = pd.to_numeric(y_test.replace({'Yes': 1, 'No': 0}), errors='coerce')\n",
    "mask = y_test_numeric.notna()\n",
    "y_test_clean = y_test_numeric[mask]\n",
    "y_pred_rf_lr_clean = y_pred_rf_lr[mask] # <-- Alinha as previsões\n",
    "\n",
    "# 4. Mostra o relatório\n",
    "print(\"\\nRandom Forest Test Classification Report [LR Features]:\")\n",
    "print(classification_report(y_test_clean, y_pred_rf_lr_clean, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d0eb28-5a11-441e-bdc2-d1db0e335d69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
